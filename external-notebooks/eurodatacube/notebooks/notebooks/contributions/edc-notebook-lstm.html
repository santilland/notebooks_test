<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>South Africa Crop Type Classification on Euro Data Cube (EDC) - Location for eodashboard notebooks</title><meta property="og:title" content="South Africa Crop Type Classification on Euro Data Cube (EDC) - Location for eodashboard notebooks"/><meta name="generator" content="mystmd"/><meta name="keywords" content=""/><link rel="stylesheet" href="/build/_assets/app-HG4THSM4.css"/><link rel="stylesheet" href="/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/favicon.ico"/><link rel="stylesheet" href="/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><a class="flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/"><span class="text-md sm:text-xl tracking-tight sm:mr-5">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R3iop:" data-state="closed" class="flex items-center h-10 aspect-square sm:w-64 text-left text-gray-400 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="hidden sm:block grow">Search</span><div aria-hidden="true" class="items-center hidden mx-1 font-mono text-sm text-gray-400 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Toggle theme between light and dark mode." aria-label="Toggle theme between light and dark mode."><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"></div><div class="hidden sm:block"></div></div></nav></div><article class="article content article-grid grid-gap"><main class="article-grid subgrid-gap col-screen"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="mb-8 pt-9"><div class="flex items-center h-6 mb-5 text-sm font-light"><div class="flex-grow"></div><a href="https://github.com/eoxhub-workspaces/documentation/" title="GitHub Repository: eoxhub-workspaces/documentation/" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a><div class="inline-block mr-1"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block"><title>Jupyter Notebook</title><path d="M20.2 1.7c0 .8-.5 1.4-1.3 1.5-.8 0-1.4-.5-1.5-1.3 0-.8.5-1.4 1.3-1.5.8-.1 1.5.5 1.5 1.3zM12 17.9c-3.7 0-7-1.3-8.7-3.3 1.8 4.8 7.1 7.3 11.9 5.5 2.5-.9 4.5-2.9 5.5-5.5-1.7 2-4.9 3.3-8.7 3.3zM12 5.1c3.7 0 7 1.3 8.7 3.3-1.8-4.8-7.1-7.3-11.9-5.5-2.5.9-4.5 2.9-5.5 5.5 1.7-2 5-3.3 8.7-3.3zM6.9 21.8c.1 1-.7 1.8-1.7 1.9-1 .1-1.8-.7-1.9-1.7 0-1 .7-1.8 1.7-1.9 1-.1 1.8.7 1.9 1.7zM3.7 4.6c-.6 0-1-.4-1-1s.4-1 1-1 1 .4 1 1c0 .5-.4 1-1 1z"></path></svg></div><div class="relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="relative ml-2 -mr-1" id="headlessui-menu-button-:Rd4fop:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem"><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="mb-0">South Africa Crop Type Classification on Euro Data Cube (EDC)</h1></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><div id="YVZz8pRPvM" class="relative group/block"><p>This notebook shows the steps towards preparing data for training a supervised machine learning model on EDC. We will train the model based on the optical bands with 10 m resolution of Sentinel-2 imagery, i.e., the B2, B3, B4, and B8.</p><p>We will use the Long Short-Term Memory model (LSTM), a variation of a recurrent neural network (RNN), which learns the temporal context of a particular yearly time series of a crop.</p><p>As for the ground truth, we will use the <a target="_blank" rel="noreferrer" href="https://collections.eurodatacube.com/south-africa-crops-competition/" class="">South Africa Crop Type Competition</a> collection, which is part of the EDC public collection and contains the field identification label representing the area of crop fields and the corresponding crop type collected via aerial and vehicle surveys.</p><p>In this example notebook, the expected outcome is a muticlass classifier that identifies different crop types planted in the fields in South Africa.</p><p>We will prepare the training data in the following steps:</p><ol start="1"><li>Search for available ground truth labels</li><li>Download features and labels</li><li>Reshape data for model training</li><li>Normalize and undersample data</li><li>Train model</li><li>Run model on validation data</li><li>Evaluate results</li></ol></div><div id="ucRSUXGgQ4" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">%load_ext autoreload
%autoreload 2</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="QB0AYAi9W2Rz0xzgNW5yD" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="F7y8lfMCfE" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import sys

print(sys.version)

# NOTE: all necessary packages are preinstalled in the EuroDataCube curated &#x27;edcg-2023.10-01&#x27; conda kernel!</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="V2SvMYcr9QxR1ssUmJxaf" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>3.10.13 | packaged by conda-forge | (main, Oct 26 2023, 18:07:37) [GCC 12.3.0]
</span></code></pre></div></div></div><div id="b4IG0xAmy7" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import datetime
import os
import pickle
from collections import defaultdict

import contextily as cx
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
import torch.nn.functional as F
from eolearn.core import (
    EOExecutor,
    EOPatch,
    EOTask,
    EOWorkflow,
    FeatureType,
    OverwritePermission,
    SaveTask,
    linearly_connect_tasks,
)
from eolearn.features.extra.interpolation import LinearInterpolationTask
from eolearn.io import SentinelHubEvalscriptTask, SentinelHubInputTask
from matplotlib.colors import BoundaryNorm, ListedColormap
from sentinelhub import (
    CRS,
    Band,
    BBox,
    DataCollection,
    SentinelHubStatistical,
    SentinelHubStatisticalDownloadClient,
    SHConfig,
    Unit,
    UtmZoneSplitter,
    bbox_to_dimensions,
)
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from torch import nn
from torch.optim import Adam
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

rng = np.random.default_rng(42)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="qI-hXnMcIMi4r6DP-a6m6" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="CE1sNrQC8O" class="relative group/block"><h2 id="id-1-search-for-available-ground-truths-labels" class="relative group"><span class="heading-text">1. Search for available ground truth’s labels</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-1-search-for-available-ground-truths-labels" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>To start preparing a training dataset, we need to find the areas which contain ground truth available in the <a target="_blank" rel="noreferrer" href="https://collections.eurodatacube.com/south-africa-crops-competition/" class="">South Africa Crop Type Competition</a> data collection on EDC. We will use the geographical coverage and the temporal availability provided on the linked webpage above to make a Catalog API request.</p></div><div id="MMjDNfbGEh" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># load the SH config
config = SHConfig()

# split the AOI into 2560m x 2560m tiles
extent = BBox((17.85, -33.089240, 18.193359, -32.7), crs=CRS.WGS84)
bbox_list = UtmZoneSplitter([extent.geometry], crs=extent.crs, bbox_size=[2560, 2560]).bbox_list</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="lfKLV1nwWZiOHa1nbPzvI" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>/home/conda/users/78c2e9a35d8f916b51e069b3359b976953e33c4ac102d2c530b5373dd8486c9d-20240215-082503-014013-382-edcg-2023.10-01/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection
  return lib.intersection(a, b, **kwargs)
/home/conda/users/78c2e9a35d8f916b51e069b3359b976953e33c4ac102d2c530b5373dd8486c9d-20240215-082503-014013-382-edcg-2023.10-01/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection
  return lib.intersection(a, b, **kwargs)
</span></code></pre></div></div></div><div id="ZYG0SMtq7Y" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">plt.style.use([&quot;default&quot;, &quot;bmh&quot;])
fig, ax = plt.subplots(figsize=(12, 5))
fig.patch.set_alpha(1)

# construct grid for plotting
grid = defaultdict(list)
for idx, bbox in enumerate(bbox_list):
    grid[bbox.crs.epsg].append({&quot;geometry&quot;: bbox.geometry, &quot;bbox_id&quot;: idx})

gdf_list = [
    gpd.GeoDataFrame(subset, geometry=&quot;geometry&quot;, crs=crs).to_crs(CRS.WGS84.epsg) for crs, subset in grid.items()
]
gdf = pd.concat(gdf_list).sort_values(&quot;bbox_id&quot;).reset_index(drop=True)

# plot grid and basemap
gdf.plot(edgecolor=&quot;k&quot;, alpha=0.2, ax=ax)
cx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="87bbW_407iQlik2aUG9GW" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/build/e33aa4016a1f9b7fc3affc4a543094a6.png" alt="&lt;Figure size 1200x500 with 1 Axes&gt;"/></div></div><div id="xU1N56krtb" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def get_coverage_request(
    bbox: BBox,
    time_interval: tuple[str, str],
    evalscript: str,
    collection: DataCollection = DataCollection.SENTINEL2_L2A,
):
    &quot;&quot;&quot;Construct request for SentinelHubStatistical data to get coverage of labels in the tile&quot;&quot;&quot;
    patch_size = bbox_to_dimensions(bbox, resolution=60)

    return SentinelHubStatistical(
        aggregation=SentinelHubStatistical.aggregation(
            evalscript=evalscript,
            time_interval=time_interval,
            aggregation_interval=&quot;P1D&quot;,
            size=patch_size,
        ),
        input_data=[SentinelHubStatistical.input_data(collection)],
        bbox=bbox,
    )


# prepare evalscript for downloading crop labels
labels_evalscript = &quot;&quot;&quot;
//VERSION=3

function setup() {
    return {
        input: [
            {&quot;bands&quot;: [&quot;crop&quot;, &quot;dataMask&quot;]}
        ],
        output: [
            {
                id: &quot;LABELS&quot;,
                bands: 1,
                sampleType: &quot;UINT8&quot;
            },
            {
                id: &quot;dataMask&quot;,
                bands: 1,
                sampleType: &quot;UINT8&quot;
            }
        ]
    };
}

function evaluatePixel(sample) {
    return {LABELS: [sample.crop], dataMask: [sample.dataMask]};
}
&quot;&quot;&quot;

# define collection which points to crop labels
collection_id = &quot;bd457670-af1b-45cd-bef2-6a7c93bf5e6e&quot;
south_africa_crop = DataCollection.define_byoc(
    collection_id, bands=[Band(name=&quot;crop&quot;, units=(Unit.DN,), output_types=(np.uint8,))], metabands=[], is_timeless=True
)

# specify labels definition time (from webpage) and run the requests
labels_time_interval = (&quot;2017-08-01&quot;, &quot;2017-08-02&quot;)
kwargs = dict(time_interval=labels_time_interval, evalscript=labels_evalscript, collection=south_africa_crop)
requests = [get_coverage_request(bbox, **kwargs).download_list[0] for bbox in bbox_list]
client = SentinelHubStatisticalDownloadClient()

# download or load data from disk
if not os.path.exists(&quot;stats.pkl&quot;):
    stats = client.download(requests)
    pickle.dump(stats, open(&quot;stats.pkl&quot;, &quot;wb&quot;))
else:
    stats = pickle.load(open(&quot;stats.pkl&quot;, &quot;rb&quot;))</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="82utOBh0w6OEYU8JqNl_2" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="uNEjUQZERY" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def extract_coverage(stats: list[dict]):
    &quot;&quot;&quot;Extract labels coverage from the statistical data&quot;&quot;&quot;
    data = stats[&quot;data&quot;][0][&quot;outputs&quot;][&quot;LABELS&quot;][&quot;bands&quot;][&quot;B0&quot;][&quot;stats&quot;]
    return 1 - data[&quot;noDataCount&quot;] / data[&quot;sampleCount&quot;]


# extract coverage from the stats
coverages = np.array([extract_coverage(s) if s[&quot;data&quot;] else 0 for s in stats])</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="hvsXNw4h0OSjRJqkepCjk" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="tfefrxzhzF" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">plt.style.use([&quot;default&quot;, &quot;bmh&quot;])
fig, ax = plt.subplots(figsize=(15, 5))
fig.patch.set_alpha(1)

# plot labels coverage over tiles, ignore tiles with no data
ax.hist(coverages[coverages &gt; 0], bins=30)
ax.set_xlabel(&quot;Reference data coverage&quot;);</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="2t72REfA--XmcZgx08w5V" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/build/9ca6b536533a2d663a7a1012cdad50b0.png" alt="&lt;Figure size 1500x500 with 1 Axes&gt;"/></div></div><div id="tpu1j4lSJG" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"></code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="8Lcl90lmFbGGp3cUn__4g" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="LnzdjwaWjf" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># let&#x27;s keep only the tiles with coverage above 40%
coverage_filter = np.array(coverages) &gt; 0.40
print(f&quot;{np.count_nonzero(coverage_filter)} eligible patches&quot;)

filtered = np.array(bbox_list)[coverage_filter]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="btO-bAdFZOHDOBxSsD9c0" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>63 eligible patches
</span></code></pre></div></div></div><div id="cBGa99Z2rL" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">plt.style.use([&quot;default&quot;, &quot;bmh&quot;])
fig, ax = plt.subplots(figsize=(12, 5))
fig.patch.set_alpha(1)

# set new column
gdf[&quot;eligible&quot;] = coverage_filter

# plot grid and basemap
gdf.plot(color=&quot;none&quot;, edgecolor=&quot;k&quot;, alpha=0.2, ax=ax)
gdf[gdf[&quot;eligible&quot;]].plot(color=&quot;C0&quot;, edgecolor=&quot;k&quot;, alpha=0.5, ax=ax)
cx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="e1lBT6Phiu525oGfRNgXd" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/build/692056e1100b97481d8ae22cda7d69a3.png" alt="&lt;Figure size 1200x500 with 1 Axes&gt;"/></div></div><div id="sw9f9flAZf" class="relative group/block"><h2 id="id-2-download-features-and-labels" class="relative group"><span class="heading-text">2. Download features and labels</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-2-download-features-and-labels" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Now we know where the groud truth is available, we use eo-learn to download the data and the labels. This is what the eo-learn workflow does:</p><ol start="1"><li>Download bands data (B, G, R, NIR)</li><li>Download labels data</li><li>Construct valid mask from data availability mask and cloud mask</li><li>Perform temporal interpolation and resample to uniform timestamps across whole AOI</li></ol></div><div id="XcePpnr93X" class="relative group/block"><p>For the purpose of this notebook, we will download only 2 EOPatches, but to properly train the model, set the <code>tutorial_mode</code> to <code>False</code> to properly train the model</p></div><div id="yzdH6nSQUI" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">tutorial_mode = True
if tutorial_mode:  # select first two eligible EOPatches
    subset = [18, 20]
    selected = filtered[subset]
    gdf[&quot;selected&quot;] = False
    gdf.loc[gdf.loc[gdf.eligible].index[subset], &quot;selected&quot;] = True
else:  # select all eligible EOPatches
    selected = filtered
    gdf[&quot;selected&quot;] = coverage_filter</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="YEb50fDMXdk6hHVYXCTy2" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="mYlWz4Cdy6" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># prepare task for joining valid data mask and cloud masks from SH
class SentinelHubValidDataTask(EOTask):
    &quot;&quot;&quot;
    Combine the downloaded cloud mask with `IS_DATA` to define a valid data mask
    The SentinelHub&#x27;s cloud mask is expected in eopatch.mask[&#x27;CLM&#x27;]
    &quot;&quot;&quot;

    def __init__(self, output_feature):
        self.output_feature = output_feature

    def execute(self, eopatch):
        eopatch[self.output_feature] = eopatch.mask[&quot;IS_DATA&quot;].astype(bool) &amp; (~eopatch.mask[&quot;CLM&quot;].astype(bool))
        return eopatch


# BAND DATA
# Add a request for S2 bands.
# Here we also do a simple filter of cloudy scenes (on tile level).
# The s2cloudless masks are requested via additional data.
band_names = [&quot;B02&quot;, &quot;B03&quot;, &quot;B04&quot;, &quot;B08&quot;]
add_data = SentinelHubInputTask(
    bands_feature=(FeatureType.DATA, &quot;BANDS&quot;),
    bands=band_names,
    resolution=10,
    maxcc=0.8,
    time_difference=datetime.timedelta(minutes=120),
    data_collection=DataCollection.SENTINEL2_L2A,
    additional_data=[(FeatureType.MASK, &quot;dataMask&quot;, &quot;IS_DATA&quot;), (FeatureType.MASK, &quot;CLM&quot;)],
    max_threads=5,
)

# LABELS DATA
# Add a request for crop labels.
add_labels = SentinelHubEvalscriptTask(
    features=(FeatureType.MASK_TIMELESS, &quot;LABELS&quot;),
    evalscript=labels_evalscript,
    data_collection=south_africa_crop,
    resolution=10,
    max_threads=5,
)

# VALIDITY MASK
# Validate pixels using SentinelHub&#x27;s cloud detection mask and region of acquisition
add_sh_validmask = SentinelHubValidDataTask((FeatureType.MASK, &quot;IS_VALID&quot;))

# LINEAR TEMPORAL INTERPOLATION
# linear interpolation of full time-series and date resampling
# needed to evaluate time series at specific dates for all data
time_interval = (&quot;2017-01-01&quot;, &quot;2017-12-31&quot;)
resampled_range = (time_interval[0], time_interval[1], 15)  # define target timestamps (every 15 days)
linear_interp = LinearInterpolationTask(
    (FeatureType.DATA, &quot;BANDS&quot;),  # name of field to interpolate
    mask_feature=(FeatureType.MASK, &quot;IS_VALID&quot;),  # mask to be used in interpolation
    copy_features=[(FeatureType.MASK_TIMELESS, &quot;LABELS&quot;)],  # features to keep
    resample_range=resampled_range,
)

# SAVING TO OUTPUT
save = SaveTask(&quot;./eopatches&quot;, overwrite_permission=OverwritePermission.OVERWRITE_FEATURES)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="tHSdQVrqztZv76PtzDgVl" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="g4naKqHYm5" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Define the workflow
workflow_nodes = linearly_connect_tasks(
    add_data,
    add_labels,
    add_sh_validmask,
    linear_interp,
    save,
)
workflow = EOWorkflow(workflow_nodes)

# Define additional parameters of the workflow
input_bands = workflow_nodes[0]
save_node = workflow_nodes[-1]
execution_args = []
for idx, bbox in enumerate(selected):
    execution_args.append(
        {
            input_bands: {&quot;bbox&quot;: bbox, &quot;time_interval&quot;: time_interval},
            save_node: {&quot;eopatch_folder&quot;: f&quot;eopatch_{idx}&quot;},
        }
    )

# Execute the workflow
if not os.path.exists(&quot;eopatches&quot;):
    executor = EOExecutor(workflow, execution_args, save_logs=True)
    executor.run(workers=2)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="I_rFH4vwSmxGT4CTH2oIk" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="lnbpXqoF0c" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># collect the data and labels over all eopatches
data_list = []
labels_list = []
for eop_idx in tqdm(range(len(selected))):
    eop = EOPatch.load(f&quot;./eopatches/eopatch_{eop_idx}&quot;)
    data_list.append(eop.data[&quot;BANDS&quot;])
    labels_list.append(eop.mask_timeless[&quot;LABELS&quot;])</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="U5j29bPI_z9UWghwwkVWj" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="bRdodOUE3Z" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># plot selected scenes in true color
ncols = 2
nrows = 1
time_idx = 12

plt.style.use([&quot;default&quot;, &quot;bmh&quot;])
fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 2, nrows * 2))
fig.patch.set_alpha(1)

for ax, image in zip(axs.flatten(), data_list):
    ax.imshow(np.clip(image[time_idx][..., [2, 1, 0]] * 3.5, 0, 1))
    ax.axis(&quot;off&quot;)

plt.tight_layout()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="MXBnwGZLsPJrIQtFGW2Wd" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="TVOo8cia4v" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># plot also labels
ncols = 2
nrows = 1

label_codes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
label_colors = [
    &quot;#ffffff&quot;,
    &quot;#a6cee3&quot;,
    &quot;#1f78b4&quot;,
    &quot;#b2df8a&quot;,
    &quot;#33a02c&quot;,
    &quot;#fb9a99&quot;,
    &quot;#e31a1c&quot;,
    &quot;#fdbf6f&quot;,
    &quot;#ff7f00&quot;,
    &quot;#cab2d6&quot;,
]
label_text = [
    &quot;No data&quot;,
    &quot;Lucerne/Medics&quot;,
    &quot;Planted pastures (perennial)&quot;,
    &quot;Fallow&quot;,
    &quot;Wine grapes&quot;,
    &quot;Weeds&quot;,
    &quot;Small grain grazing&quot;,
    &quot;Wheat&quot;,
    &quot;Canola&quot;,
    &quot;Rooibos&quot;,
]

plt.style.use([&quot;default&quot;, &quot;bmh&quot;])
fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 2, nrows * 2))
fig.patch.set_alpha(1)

cmap = ListedColormap(label_colors)
norm = BoundaryNorm(label_codes, 10)

for ax, image in zip(axs.flatten(), labels_list):
    ax.imshow(image.squeeze(-1), cmap=cmap, norm=norm, interpolation=&quot;none&quot;)
    ax.axis(&quot;off&quot;)

plt.tight_layout()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="GBiU6GamgoFglG4A_cP8F" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="gkwmGkWObj" class="relative group/block"><h2 id="id-3-reshape-data-for-model-training" class="relative group"><span class="heading-text">3. Reshape data for model training</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-3-reshape-data-for-model-training" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>The data for the LSTM model needs to be in the form of <code>N x T x C</code>, where <em>N</em> represents the number of pixels, <em>T</em> represents the number of timestamps, and <em>C</em> the number of channels.</p><p>We have 25 timestamps and 4 channels (B, G, R, NIR).</p></div><div id="ZWW3Mm8cxj" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># split tiles into train and validation patches
train_slice = slice(None, None, 2)  # evens
val_slice = slice(1, None, 2)  # odds

train_data, train_labels = data_list[train_slice], labels_list[train_slice]
validate_data, validate_labels = data_list[val_slice], labels_list[val_slice]

sel_gdf = gdf[gdf.selected].copy()
sel_gdf[&quot;train_split&quot;] = False
sel_gdf.loc[train_slice, &quot;train_split&quot;] = True

plt.style.use([&quot;default&quot;, &quot;bmh&quot;])
fig, ax = plt.subplots(figsize=(12, 5))
fig.patch.set_alpha(1)

# plot grid and basemap
gdf.plot(color=&quot;none&quot;, edgecolor=&quot;k&quot;, alpha=0.2, ax=ax)
sel_gdf[sel_gdf.train_split].plot(color=&quot;xkcd:green&quot;, edgecolor=&quot;k&quot;, alpha=0.2, ax=ax)
sel_gdf[~sel_gdf.train_split].plot(color=&quot;xkcd:brick&quot;, edgecolor=&quot;k&quot;, alpha=0.2, ax=ax)
cx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="lSsA9tlHVefexYYEe9ML0" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="jCbiKkiWc5" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># function for extracting and reshaping the data and labels
def get_model_input(data: np.ndarray, labels: np.ndarray) -&gt; tuple:
    t, h, w, d = data.shape
    x = np.reshape(np.moveaxis(data, 0, -2), (h * w, t, d))  # N x T x C
    y = np.reshape(labels, (h * w))
    return x[y != 0], y[y != 0]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="U3nysOptIO-WA-mLBCLo-" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="eYbZmyJV2Y" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># reshape for train data
x_train, y_train = [], []
for data, labels in zip(train_data, train_labels):
    model_input = get_model_input(data, labels)
    x_train.append(model_input[0])
    y_train.append(model_input[1])

x_train = np.concatenate(x_train, axis=0).astype(np.float32)
y_train = np.concatenate(y_train, axis=0).astype(np.uint8)
del train_data, train_labels

print(x_train.shape)
print(y_train.shape)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="qv1omFrWtht8A6fCVO_Bl" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="cqBowsjSti" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># reshape for validation data
x_validate, y_validate = [], []
for data, labels in zip(validate_data, validate_labels):
    model_input = get_model_input(data, labels)
    x_validate.append(model_input[0])
    y_validate.append(model_input[1])

x_validate = np.concatenate(x_validate, axis=0).astype(np.float32)
y_validate = np.concatenate(y_validate, axis=0).astype(np.uint8)
del validate_data, validate_labels

print(x_validate.shape)
print(y_validate.shape)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="1Z73lEpcMzrffoIUNe02r" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="XbzhsxjH4Z" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">plt.style.use([&quot;default&quot;, &quot;bmh&quot;])
fig, ax = plt.subplots(figsize=(15, 5), ncols=2, sharex=True)
fig.patch.set_alpha(1)

labels_dict = dict(zip(label_codes, label_text))

labels, counts = np.unique(y_train, return_counts=True)
ax[0].barh(range(len(labels)), counts)
ax[0].set_title(&quot;Train data labels distribution&quot;)
ax[0].set_yticks(range(len(labels)), [labels_dict[lbl] for lbl in labels])

labels, counts = np.unique(y_validate, return_counts=True)
ax[1].barh(range(len(labels)), counts)
ax[1].set_title(&quot;Validation data labels distribution&quot;)
ax[1].set_yticks(range(len(labels)), [labels_dict[lbl] for lbl in labels])

plt.tight_layout()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="bMWLdtXrTL3oIDU4U0lRp" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="EFy3P9V4YY" class="relative group/block"><h2 id="id-4-normalize-and-undersample-data" class="relative group"><span class="heading-text">4. Normalize and undersample data</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-4-normalize-and-undersample-data" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>For the columns that contain numerical data, we should ensure the data range is normalized in case the data has different scales. This process may improve the models performance, and also preserve outliers.</p><p>Here we will use a simple method of undersampling just to minimize the amount of huge data we are handling, but the balance of the classes will remain the same.</p></div><div id="TYfpWBHNYZ" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># set random seet
np.random.seed(42)

# define percentage of data to use
percentage = 0.3

# sample train data
sample_mask = np.random.choice(x_train.shape[0], int(percentage * x_train.shape[0]))
class_counts_before = np.unique(y_train, return_counts=True)
x_train = x_train[np.sort(sample_mask)]
y_train = y_train[np.sort(sample_mask)]
class_counts_after = np.unique(y_train, return_counts=True)

# sample validation data
sample_mask_val = np.random.choice(x_validate.shape[0], int(percentage * x_validate.shape[0]))
x_validate = x_validate[sample_mask_val]
y_validate = y_validate[sample_mask_val]

# normalize train data
n, t, c = x_train.shape
x_train = StandardScaler().fit_transform(x_train.reshape(n * t, c)).reshape(n, t, c)

# normalize validation data
n, t, c = x_validate.shape
val_scaler = StandardScaler().fit(x_validate.reshape(n * t, c))
x_validate = val_scaler.transform(x_validate.reshape(n * t, c)).reshape(n, t, c)

# check shapes
x_train.shape, x_validate.shape</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="AFvWvs8QZcSokwIMVzeGe" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="DGpz7xaRKr" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># fill nan values with -1
x_train = np.nan_to_num(x_train, nan=-1)
x_validate = np.nan_to_num(x_validate, nan=-1)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="CGvL8wZrJ4dJoO4FGkHmB" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="hI3JTEJTj0" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># check labels distribution before and after undersampling
plt.style.use([&quot;default&quot;, &quot;bmh&quot;])
fig, axs = plt.subplots(figsize=(12, 7), ncols=2)
fig.patch.set_alpha(1)

labels_dict = dict(zip(label_codes, label_text))
colors_dict = dict(zip(label_codes, label_colors))

axs[0].pie(
    class_counts_before[1],
    labels=[labels_dict[lbl] for lbl in class_counts_before[0]],
    autopct=&quot;%1.1f%%&quot;,
    colors=[colors_dict[lbl] for lbl in class_counts_before[0]],
    wedgeprops={&quot;linewidth&quot;: 2, &quot;edgecolor&quot;: &quot;white&quot;, &quot;alpha&quot;: 0.7},
)
axs[0].set_title(&quot;Class Distribution Before Undersampling&quot;)

axs[1].pie(
    class_counts_after[1],
    labels=[labels_dict[lbl] for lbl in class_counts_after[0]],
    autopct=&quot;%1.1f%%&quot;,
    colors=[colors_dict[lbl] for lbl in class_counts_after[0]],
    wedgeprops={&quot;linewidth&quot;: 2, &quot;edgecolor&quot;: &quot;white&quot;, &quot;alpha&quot;: 0.7},
)
axs[1].set_title(&quot;Class Distribution After Undersampling&quot;)

plt.tight_layout()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="-OqP5BoD857rxWDHeyaSW" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="gkB0d4o5n8" class="relative group/block"><h2 id="id-5-train-the-model" class="relative group"><span class="heading-text">5. Train the model</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-5-train-the-model" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Now it’s time to construct and train the LSTM model. First we need a bunch of classes that the model will use, from the dataloaders to the model itself.</p></div><div id="czP4KYYUbu" class="relative group/block"><h4 id="define-the-data-loaders" class="relative group"><span class="heading-text">Define the data loaders</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#define-the-data-loaders" title="Link to this Section" aria-label="Link to this Section">¶</a></h4></div><div id="LukNKMPBtZ" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># class for the dataloader
class CustomImageDataset(Dataset):
    def __init__(self, x_data: np.ndarray, y_data: np.ndarray, encoder: LabelEncoder):
        self.x_data = x_data
        self.y_data = encoder.transform(y_data)

    def __len__(self):
        return len(self.y_data)

    def __getitem__(self, idx):
        return self.x_data[idx], self.y_data[idx]


# encode labels to 0 - num_labels
encoder = LabelEncoder().fit(list(set(y_train) | set(y_validate)))
for lbl in encoder.classes_:
    print(f&quot;{lbl} -&gt; {encoder.transform([lbl])[0]}&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="pFvdR86WT9ERrTljCGtX7" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="iphdLKY8Vx" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># define the batch size and the dataloaders for train/validation data
BATCH_SIZE = 1024 if tutorial_mode else 4096
train_dataloader = DataLoader(CustomImageDataset(x_train, y_train, encoder), batch_size=BATCH_SIZE, shuffle=True)
validation_dataloader = DataLoader(
    CustomImageDataset(x_validate, y_validate, encoder), batch_size=BATCH_SIZE, shuffle=True
)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="gIKY9RuGNed97osRf_lEn" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="y3ES0RMYTk" class="relative group/block"><h3 id="define-the-model-architecture" class="relative group"><span class="heading-text">Define the model architecture</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#define-the-model-architecture" title="Link to this Section" aria-label="Link to this Section">¶</a></h3></div><div id="T5XkMx9cqw" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">class AverageMetric:
    &quot;&quot;&quot;
    Simple class for averaging metrics.
    &quot;&quot;&quot;

    def __init__(self):
        self.values = list()

    def add(self, new):
        self.values.append(new)

    def get(self):
        return np.array(self.values).mean()


# This code is taken from https://github.com/TUM-LMF/BreizhCrops
class LSTM(nn.Module):
    &quot;&quot;&quot;
    Implementation of the LSTM model for classification of a input sequence into n classes.

    :param input_dim: number of input features entering LSTM cell (i.e. 13 if all S2 bands are used)
    :type input_dim: int
    :param n_classes: number of classes
    :type n_classes: int (class labels have to be between 0 and n_classes-1)
    :param hidden_dims: The number of features in the hidden state `h`
    :type hidden_dims: int
    :param num_rnn_layers: Number of recurrent layers. E.g., setting ``num_layers=2``
                           would mean stacking two LSTMs together to form a `stacked LSTM`,
                           with the second LSTM taking in outputs of the first LSTM and
                           computing the final results.
    :type num_rnn_layers: int
    :param dropout: If non-zero, introduces a `Dropout` layer on the outputs of each
                    LSTM layer except the last layer, with dropout probability equal to
                    :attr:`dropout`.
    :type dropout: float (between 0.0 and 1.0)
    :param bidirectional: If ``True``, becomes a bidirectional LSTM.
    :type bidirectional: bool
    :param use_batchnorm: If ``True``, use batch normalization.
    :type use_batchnorm: bool
    :param use_layernorm: If ``True``, applies Layer Normalization over a mini-batch of inputs.
    :type use_layernorm: bool
    &quot;&quot;&quot;

    def __init__(
        self,
        input_dim,
        n_classes,
        hidden_dims,
        num_rnn_layers=1,
        dropout=0,
        bidirectional=False,
        use_batchnorm=False,
        use_layernorm=True,
    ):
        super().__init__()

        self.nclasses = n_classes
        self.use_batchnorm = use_batchnorm
        self.use_layernorm = use_layernorm

        self.d_model = num_rnn_layers * hidden_dims
        if use_layernorm:
            self.inlayernorm = nn.LayerNorm(input_dim)
            self.clayernorm = nn.LayerNorm((hidden_dims + hidden_dims * bidirectional) * num_rnn_layers)

        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dims,
            num_layers=num_rnn_layers,
            bias=False,
            batch_first=True,
            dropout=dropout,
            bidirectional=bidirectional,
        )

        if bidirectional:
            hidden_dims = hidden_dims * 2

        self.linear_class = nn.Linear(hidden_dims * num_rnn_layers, n_classes, bias=True)

        if use_batchnorm:
            self.bn = nn.BatchNorm1d(hidden_dims)

    def _logits(self, x):
        x = x.transpose(1, 2)

        if self.use_layernorm:
            x = self.inlayernorm(x)

        outputs, last_state_list = self.lstm.forward(x)

        # TODO: check what is goinig on here
        if self.use_batchnorm:
            b, t, d = outputs.shape
            o_ = outputs.view(b, -1, d).permute(0, 2, 1)
            outputs = self.bn(o_).permute(0, 2, 1).view(b, t, d)

        h, c = last_state_list

        nlayers, batchsize, n_hidden = c.shape
        # TODO: shouldn&#x27;t this be executed only uf layernorm is True
        h = self.clayernorm(c.transpose(0, 1).contiguous().view(batchsize, nlayers * n_hidden))
        logits = self.linear_class.forward(h)

        return logits

    def forward(self, x):
        logits = self._logits(x)
        logprobabilities = F.log_softmax(logits, dim=-1)
        return logprobabilities

    def save(self, path=&quot;model.pth&quot;, **kwargs):
        model_state = self.state_dict()
        os.makedirs(os.path.dirname(path), exist_ok=True)
        torch.save(dict(model_state=model_state, **kwargs), path)

    def load(self, path):
        snapshot = torch.load(path, map_location=&quot;cpu&quot;)
        model_state = snapshot.pop(&quot;model_state&quot;, snapshot)
        self.load_state_dict(model_state)
        return snapshot

    def update_and_freeze_body(self, pretrained, freeze_layers=(0, -1)):
        self.inlayernorm = pretrained.inlayernorm
        self.clayernorm = pretrained.clayernorm
        self.lstm = pretrained.lstm

        for child in list(self.children())[freeze_layers[0] : freeze_layers[1]]:
            for param in child.parameters():
                param.requires_grad = False

    def unfreeze(self):
        for child in self.children():
            for param in child.parameters():
                param.requires_grad = True


def train(model, optimizer, train_loader, validation_loader, epochs):
    &quot;&quot;&quot;
    Vanilla function to run the inference on all train samples given in training dataloader.
    At each epoch end the loss and accuracy of samples from the validation loader are printed.
    &quot;&quot;&quot;
    model.train()

    if torch.cuda.is_available():
        model = model.cuda()

    for epoch in range(epochs):
        # train phase
        train_loss_log = AverageMetric()
        model.train()
        for data in train_loader:
            optimizer.zero_grad()

            inputs, targets = data

            if torch.cuda.is_available():
                inputs = inputs.cuda()
                targets = targets.cuda()

            logprobabilities = model.forward(inputs.transpose(1, 2))

            loss = torch.nn.functional.nll_loss(logprobabilities, targets)
            train_loss_log.add(loss.cpu().detach().numpy())

            loss.backward()
            optimizer.step()

        # val phase
        val_loss_log = AverageMetric()
        model.eval()
        predictions_list = list()
        targets_list = list()
        for _, data in enumerate(validation_loader):
            inputs, targets = data

            if torch.cuda.is_available():
                inputs = inputs.cuda()
                targets = targets.cuda()

            logprobabilities = model.forward(inputs.transpose(1, 2))
            loss = torch.nn.functional.nll_loss(logprobabilities, targets)
            val_loss_log.add(loss.cpu().detach().numpy())

            targets_list.append(targets.cpu().detach().numpy())
            predictions_list.append((logprobabilities.cpu().detach().numpy()).argmax(1))

        val_acc = accuracy_score(np.concatenate(targets_list), np.concatenate(predictions_list))
        print(
            f&quot;Epoch {epoch}: train loss {train_loss_log.get():.3f} | &quot;
            f&quot;val loss {val_loss_log.get():.3f} | &quot;
            f&quot;val acc = {val_acc:.3f}&quot;
        )

    return model</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="vpYzymOsJTY7-zvG93HF-" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="Yun0zr18TQ" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># set up the model
model = LSTM(
    input_dim=x_train.shape[-1],
    n_classes=len(encoder.classes_),
    hidden_dims=128,
    num_rnn_layers=3,
    dropout=0.1,
    bidirectional=True,
    use_batchnorm=False,
    use_layernorm=True,
)

# set up the optimizer
optimizer = Adam(model.parameters(), lr=5e-5)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="F2NMNpxV-ed9ZeebtlS4q" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="AOmILQdelf" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">model_path = &quot;./model.pth&quot;
if not os.path.exists(model_path):
    n_epochs = 2 if tutorial_mode else 50
    # train the model for a few epochs and save when training is done
    model = train(model, optimizer, train_dataloader, validation_dataloader, epochs=n_epochs)
    model.save(model_path)
else:
    # or load the saved model from path
    model = LSTM(
        input_dim=x_train.shape[-1],
        n_classes=len(encoder.classes_),
        hidden_dims=128,
        num_rnn_layers=3,
        dropout=0.1,
        bidirectional=True,
        use_batchnorm=False,
        use_layernorm=True,
    )
    model.load(model_path)
    model.eval()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="MIhMkaoLIJnsYbwpuaJYc" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="DzmDXBHcnF" class="relative group/block"><h2 id="id-6-run-the-model-on-validation-data" class="relative group"><span class="heading-text">6. Run the model on validation data</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-6-run-the-model-on-validation-data" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Now that the model is trained, let’s run it on the validation data and see how well it performs.</p></div><div id="oDink2ETQ6" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># convert validation data to the proper format for inference
n_samples = 2000 if tutorial_mode else 10000
val_sample_mask = np.random.choice(range(len(x_validate)), n_samples)
val_tensor = torch.from_numpy(x_validate[val_sample_mask]).transpose(1, 2)
logprobas = model.forward(val_tensor).cpu().detach().numpy()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="hFbMw2x2LTEH7Sw8k-o4v" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="xhLZ7JXnx0" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># calculate the confidence and predictions
probas = np.exp(logprobas)
predictions = encoder.classes_[probas.argmax(axis=1)]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="_kVALj6KuyLvJbrGhxv6F" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="obLTfuYJbD" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Calculate Accuracy, Precision and Recall, F1-Score
y_true = y_validate[val_sample_mask]
labels = sorted(set(y_true) | set(predictions))

accuracy = accuracy_score(y_true, predictions)
precision = precision_score(y_true, predictions, average=None, labels=labels)
recall = recall_score(y_true, predictions, average=None, labels=labels)
f1 = f1_score(y_true, predictions, average=None, labels=labels)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="bpWeLAD522afat6v-FNgt" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="IM7I9DwWyb" class="relative group/block"><p>Plot F1-score per label</p></div><div id="ylE316aOKo" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">fig, ax = plt.subplots(figsize=(12, 5))

# calculate F1 score dataframe
df_f1 = pd.DataFrame({&quot;Code&quot;: lbl, &quot;Label&quot;: [labels_dict[lbl] for lbl in labels], &quot;F1 score&quot;: f1})

# plot scores
ax.barh(df_f1[&quot;Label&quot;], df_f1[&quot;F1 score&quot;])

ax.set_title(&quot;Labels F1-Score&quot;)
ax.set_xlabel(&quot;F1-Score&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="JrYnT-VBh7PyHIuEchCQ9" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="Z6Ih6anNAQ" class="relative group/block"><p>Plot confusion matrix</p></div><div id="ywnm4zpKun" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">true_labels_counts_dict = dict(zip(*np.unique(y_validate[val_sample_mask].tolist(), return_counts=True)))
counts = [true_labels_counts_dict.get(lbl, 0) for lbl in labels]

labels = np.array(labels)[np.argsort(counts)[::-1]]
counts = sorted(counts)[::-1]

cm = confusion_matrix(y_true, predictions, normalize=&quot;true&quot;, labels=labels)
ax = sns.heatmap(
    cm,
    annot=True,
    cmap=&quot;Greens&quot;,
    fmt=&quot;.2f&quot;,
    xticklabels=[labels_dict[lbl] for lbl in labels],
    yticklabels=[f&quot;{labels_dict[lbl]}\n({c})&quot; for lbl, c in zip(labels, counts)],
    cbar=False,
    annot_kws={&quot;color&quot;: &quot;black&quot;},
)
ax.set_xlabel(&quot;Predicted labes&quot;)
ax.set_ylabel(&quot;True labels&quot;)
ax.set_title(&quot;Confusion matrix&quot;)
plt.show()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="UhdYt3Sq7bUgVSP9gDBsy" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="W519IPvGxc" class="relative group/block"><p>Plot time series according to the true label</p></div><div id="Eyb5eWJZGD" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">ncols = 3
nrows = len(encoder.classes_) // ncols + 1

fig, axs = plt.subplots(figsize=(15, 7), nrows=nrows, ncols=ncols)

n, t, c = x_validate.shape
plot_data = val_scaler.inverse_transform(x_validate.reshape(n * t, c)).reshape(n, t, c)[val_sample_mask]
ndvi = (plot_data[..., 3] - plot_data[..., 2]) / (plot_data[..., 3] + plot_data[..., 2])

for idx, (ax, cc) in enumerate(zip(axs.ravel(), encoder.classes_)):
    ax.plot(ndvi[y_true == cc][:200].T, color=f&quot;C{idx}&quot;, alpha=0.1)
    ax.set_title(f&quot;Crop Type {cc} ({labels_dict[cc]})&quot;)

plt.tight_layout()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="1DQDveKiSMGeugzyKzNNh" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="hi0G9bP318" class="relative group/block"><p>Plot time series according to the predicted label</p></div><div id="kyP0RcgJaR" class="relative group/block"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre class="block p-3 hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">ncols = 3
nrows = len(encoder.classes_) // ncols + 1

fig, axs = plt.subplots(figsize=(15, 7), nrows=nrows, ncols=ncols)

for idx, (ax, cc) in enumerate(zip(axs.ravel(), encoder.classes_)):
    ax.plot(ndvi[predictions == cc][:200].T, color=f&quot;C{idx}&quot;, alpha=0.1)
    ax.set_title(f&quot;Crop Type {cc} ({labels_dict[cc]})&quot;)

plt.tight_layout()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="7Kr5qMCHSJUq2bjouXVK2" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div></div></main></article><script>((a,d)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let f=JSON.parse(sessionStorage.getItem(a)||"{}")[d||window.history.state.key];typeof f=="number"&&window.scrollTo(0,f)}catch(h){console.error(h),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/build/entry.client-UNPC4GT3.js"/><link rel="modulepreload" href="/build/_shared/chunk-OCTKKCIL.js"/><link rel="modulepreload" href="/build/_shared/chunk-UAI5KRM7.js"/><link rel="modulepreload" href="/build/_shared/chunk-2NH4LW52.js"/><link rel="modulepreload" href="/build/_shared/chunk-IZFMW3M4.js"/><link rel="modulepreload" href="/build/_shared/chunk-HBJK6BW3.js"/><link rel="modulepreload" href="/build/_shared/chunk-HYMQ7M2K.js"/><link rel="modulepreload" href="/build/_shared/chunk-OHOXABTA.js"/><link rel="modulepreload" href="/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/build/_shared/chunk-CPTH56EW.js"/><link rel="modulepreload" href="/build/_shared/chunk-3CVK3PYF.js"/><link rel="modulepreload" href="/build/_shared/chunk-J6FHCSRC.js"/><link rel="modulepreload" href="/build/_shared/chunk-S4SWV34C.js"/><link rel="modulepreload" href="/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/build/root-QGLRP2PL.js"/><link rel="modulepreload" href="/build/_shared/chunk-CB3BH7WY.js"/><link rel="modulepreload" href="/build/routes/$-7JYT5576.js"/><script>window.__remixContext = {"url":"/external-notebooks/eurodatacube/notebooks/notebooks/contributions/edc-notebook-lstm","state":{"loaderData":{"root":{"config":{"version":1,"myst":"1.3.25","options":{"hide_toc":true,"hide_footer_links":true,"folders":true},"nav":[],"actions":[],"projects":[{"github":"https://github.com/eoxhub-workspaces/documentation/","id":"5aa0318b-5bc1-41e2-b1d4-053e8b61961b","exports":[],"bibliography":[],"title":"Location for eodashboard notebooks","index":"readme","pages":[{"title":"External Notebooks","level":1},{"title":"Eurodatacube","level":2},{"title":"Notebooks","level":3},{"slug":"external-notebooks.eurodatacube.notebooks.readme","title":"Jupyter notebooks","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":4},{"title":"Notebooks","level":4},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.readme","title":"Euro Data Cube Notebooks","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":5},{"title":"Contributions","level":5},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-ai4arctic-sea-ice","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-biomass-forest-observation-sys","title":"AIREO pilot dataset - Biomass (Forest Observation System)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-cap","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-spacenet7","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aot-covid19","title":"Aerosol Optical Thickness (AOT) Anomaly Detection Using Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.batch-zarr-output","title":"Generating Zarr data cubes with Sentinel Hub Batch API","description":"","date":"","thumbnail":"/build/68dffbf744fa42f4bf3d35e55684984e.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.client-side-processing-using-mass-sh","title":"Important notes","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.cmems-data-download","title":"Cmems Data Download","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.detect-trucks-sentinel2","title":"Detect Trucks using Sentinel-2 data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.drought-monitoring","title":"Drought impact monitoring platform","description":"","date":"","thumbnail":"/build/ad5c19782b006f72352ae57794305840.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-africa-cube-africa-custom-contest","title":"Custom Script Contest – Urban Growth in Africa","description":"","date":"","thumbnail":"/build/e9275f3fe80a704e2f5aa1ec1220e48a.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-byoc-tutorial","title":"How to bring your own data to EDC: Using Sentinel Hub Python package","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-demo-phi-week-v2","title":"xcube-gen and xcube-geodb Hands-On Workshop Φ-Week 2020","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-geodb-4-eurocrops-demo","title":"Demonstration of Eurocrops data use in geoDB","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-notebook-lstm","title":"South Africa Crop Type Classification on Euro Data Cube (EDC)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-usecase-lpis-crop-type-classification","title":"Important notes","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.eurocrops-crop-classification-example","title":"Crop-classification using Sentinel-2 time-series","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.ghsl-urban-delineation","title":"\u003cb\u003eUrban delineation\u003cb\u003e","description":"","date":"","thumbnail":"/build/7bbf545a5eae1d9ebdb91ae42b261063.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.globalearthmonitor-example","title":"Pre-requisites","description":"","date":"","thumbnail":"/build/205c1ded6bd59771975a9fbd3753fd3b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.gpu-support-eoxhub","title":"GPU-Support on EOxHub","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.gran-chaco-phiweek","title":"ESA EO \\phi- week 2020","description":"","date":"","thumbnail":"/build/80cee7db6cdc7002ceedda5689f889fa.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.howtolulc-batch-updated-210621","title":"How To: Land-Use-Land-Cover Prediction for Slovenia - using the Batch Processing API","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.lpis-lulc-slo","title":"LPIS Use Case for Land Use / Land Cover Classification","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.lps2022-leverage-euro-data-cube-services","title":"Living Planet Symposium 2022 Training Session","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.no2-analysis-covid19-lockdowns","title":"Impact of Covid19 induced lockdown on NO2 levels","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.no2-so2-trend-veda-api-igarss-2023","title":"Using the NASA VEDA EOAPI","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.ogc-edc","title":"Important notes","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.openeo-client-demo","title":"openEO Client Demo","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.race-finishedgoodsinventory-inidcator","title":"Finished Goods Inventory indicator for RACE","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.third-party-planetscope","title":"Third Party Data Import - PlanetScope","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.xcube-access-datasets","title":"Using xcube to access non-commercial and commercial data sets","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.xcube-viewer-as-a-service-in-edc1","title":"Visualizing data with xcube viewer in EuroDataCube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-evaluation-workshop","title":"DAPA Evaluation Workshop: Introduction \u0026 Documentation","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-1-cube-sentinel-2","title":"DAPA Tutorial #1: Cube - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-2-area-sentinel-2","title":"DAPA Tutorial #2: Area - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-3-timeseries-sentinel-2","title":"DAPA Tutorial #3: Timeseries - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-4-value-sentinel-2","title":"DAPA Tutorial #4: Value - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-5-dem","title":"DAPA Tutorial #5: DEM","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.fairicube.fairicube-data-access-demonstration","title":"EDC Sentinel Hub - data access using xcube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-australian-bushfires","title":"Australian Bushfires","description":"","date":"","thumbnail":"/build/ab194b4f8c40a2b9d64d8a85e37ce1fd.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":["IGARSS-22","EO Dashboard"],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-data-access","title":"Data Access","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-detect-trucks-sentinel2","title":"Truck Detection Exercise","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-lockdown-in-venice","title":"Environmental Impacts of Lockdown in Venice, Italy during 2020","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-nasa-api-lockdown-in-venice","title":"Lockdown in Venice","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-no2-timeseries-analysis","title":"NO2 timeseries analysis","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-timeseries-stac-api-single-vs-multi","title":"Time series using STAC API statistics endpoints","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"title":"Curated","level":5},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.access-iacs-data","title":"How to access IACS Spatial Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-access-cci-data","title":"Access CCI data with xcube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-first-steps","title":"First steps on the Euro Data Cube platform","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-1-manage-datasets-v11","title":"Manage Collections in your GeoDB","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-2-explore-datasets-2","title":"Exploring Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-3-share-datasets1","title":"Sharing Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-access-urban-atlas-data-v1","title":"[xcube-geodb] How to accesss EEA Urban Atlas Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-data-access","title":"EDC Sentinel Hub - data access using xcube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinelhub-datafusion-basic","title":"EDC Sentinel Hub: Data Fusion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinelhub-datafusion-ndvi","title":"EDC Sentinel Hub: Data Fusion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-datasets","title":"EDC Sentinel Hub - using xcube to access different data sets in Sentinel Hub","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-setup","title":"Setup for EDC core API access using xcube client library","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-xcube-integration","title":"EDC Sentinel Hub - XCUBE integration","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-usecase-ndvi-timeline","title":"NDVI timeline","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.eo-learn","title":"Using eo-learn in EDC: a starter’s guide.","description":"","date":"","thumbnail":"/build/5a1f8475f0c3652f4630d03d72891b98.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.esdl-edc-v0-2","title":"Earth System Data Lab Tutorial @ Euro Data Cube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.exploring-time-and-space-with-edc","title":"Exploring Time and Space: A guide to accessing, analysing and visualising data in the Euro Data Cube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.how-to-access-dem-data-through-sentinel-hub-api","title":"How to access DEM data through Sentinel Hub API","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.sentinelhub-py","title":"Using Sentinel Hub Process API in EDC: a starter’s guide.","description":"","date":"","thumbnail":"/build/01e22903839391bc356ac886ec2b2fa7.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"title":"Notebooks","level":1},{"slug":"notebooks.example1","title":"📘 Example Notebook 1","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.example2","title":"📘 Example Notebook 2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static"},"routes/$":{"config":{"version":1,"myst":"1.3.25","options":{"hide_toc":true,"hide_footer_links":true,"folders":true},"nav":[],"actions":[],"projects":[{"github":"https://github.com/eoxhub-workspaces/documentation/","id":"5aa0318b-5bc1-41e2-b1d4-053e8b61961b","exports":[],"bibliography":[],"title":"Location for eodashboard notebooks","index":"readme","pages":[{"title":"External Notebooks","level":1},{"title":"Eurodatacube","level":2},{"title":"Notebooks","level":3},{"slug":"external-notebooks.eurodatacube.notebooks.readme","title":"Jupyter notebooks","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":4},{"title":"Notebooks","level":4},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.readme","title":"Euro Data Cube Notebooks","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":5},{"title":"Contributions","level":5},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-ai4arctic-sea-ice","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-biomass-forest-observation-sys","title":"AIREO pilot dataset - Biomass (Forest Observation System)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-cap","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-spacenet7","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aot-covid19","title":"Aerosol Optical Thickness (AOT) Anomaly Detection Using Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.batch-zarr-output","title":"Generating Zarr data cubes with Sentinel Hub Batch API","description":"","date":"","thumbnail":"/build/68dffbf744fa42f4bf3d35e55684984e.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.client-side-processing-using-mass-sh","title":"Important notes","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.cmems-data-download","title":"Cmems Data Download","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.detect-trucks-sentinel2","title":"Detect Trucks using Sentinel-2 data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.drought-monitoring","title":"Drought impact monitoring platform","description":"","date":"","thumbnail":"/build/ad5c19782b006f72352ae57794305840.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-africa-cube-africa-custom-contest","title":"Custom Script Contest – Urban Growth in Africa","description":"","date":"","thumbnail":"/build/e9275f3fe80a704e2f5aa1ec1220e48a.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-byoc-tutorial","title":"How to bring your own data to EDC: Using Sentinel Hub Python package","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-demo-phi-week-v2","title":"xcube-gen and xcube-geodb Hands-On Workshop Φ-Week 2020","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-geodb-4-eurocrops-demo","title":"Demonstration of Eurocrops data use in geoDB","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-notebook-lstm","title":"South Africa Crop Type Classification on Euro Data Cube (EDC)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-usecase-lpis-crop-type-classification","title":"Important notes","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.eurocrops-crop-classification-example","title":"Crop-classification using Sentinel-2 time-series","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.ghsl-urban-delineation","title":"\u003cb\u003eUrban delineation\u003cb\u003e","description":"","date":"","thumbnail":"/build/7bbf545a5eae1d9ebdb91ae42b261063.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.globalearthmonitor-example","title":"Pre-requisites","description":"","date":"","thumbnail":"/build/205c1ded6bd59771975a9fbd3753fd3b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.gpu-support-eoxhub","title":"GPU-Support on EOxHub","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.gran-chaco-phiweek","title":"ESA EO \\phi- week 2020","description":"","date":"","thumbnail":"/build/80cee7db6cdc7002ceedda5689f889fa.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.howtolulc-batch-updated-210621","title":"How To: Land-Use-Land-Cover Prediction for Slovenia - using the Batch Processing API","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.lpis-lulc-slo","title":"LPIS Use Case for Land Use / Land Cover Classification","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.lps2022-leverage-euro-data-cube-services","title":"Living Planet Symposium 2022 Training Session","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.no2-analysis-covid19-lockdowns","title":"Impact of Covid19 induced lockdown on NO2 levels","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.no2-so2-trend-veda-api-igarss-2023","title":"Using the NASA VEDA EOAPI","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.ogc-edc","title":"Important notes","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.openeo-client-demo","title":"openEO Client Demo","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.race-finishedgoodsinventory-inidcator","title":"Finished Goods Inventory indicator for RACE","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.third-party-planetscope","title":"Third Party Data Import - PlanetScope","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.xcube-access-datasets","title":"Using xcube to access non-commercial and commercial data sets","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.xcube-viewer-as-a-service-in-edc1","title":"Visualizing data with xcube viewer in EuroDataCube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-evaluation-workshop","title":"DAPA Evaluation Workshop: Introduction \u0026 Documentation","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-1-cube-sentinel-2","title":"DAPA Tutorial #1: Cube - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-2-area-sentinel-2","title":"DAPA Tutorial #2: Area - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-3-timeseries-sentinel-2","title":"DAPA Tutorial #3: Timeseries - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-4-value-sentinel-2","title":"DAPA Tutorial #4: Value - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-5-dem","title":"DAPA Tutorial #5: DEM","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.fairicube.fairicube-data-access-demonstration","title":"EDC Sentinel Hub - data access using xcube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-australian-bushfires","title":"Australian Bushfires","description":"","date":"","thumbnail":"/build/ab194b4f8c40a2b9d64d8a85e37ce1fd.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":["IGARSS-22","EO Dashboard"],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-data-access","title":"Data Access","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-detect-trucks-sentinel2","title":"Truck Detection Exercise","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-lockdown-in-venice","title":"Environmental Impacts of Lockdown in Venice, Italy during 2020","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-nasa-api-lockdown-in-venice","title":"Lockdown in Venice","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-no2-timeseries-analysis","title":"NO2 timeseries analysis","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-timeseries-stac-api-single-vs-multi","title":"Time series using STAC API statistics endpoints","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"title":"Curated","level":5},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.access-iacs-data","title":"How to access IACS Spatial Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-access-cci-data","title":"Access CCI data with xcube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-first-steps","title":"First steps on the Euro Data Cube platform","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-1-manage-datasets-v11","title":"Manage Collections in your GeoDB","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-2-explore-datasets-2","title":"Exploring Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-3-share-datasets1","title":"Sharing Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-access-urban-atlas-data-v1","title":"[xcube-geodb] How to accesss EEA Urban Atlas Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-data-access","title":"EDC Sentinel Hub - data access using xcube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinelhub-datafusion-basic","title":"EDC Sentinel Hub: Data Fusion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinelhub-datafusion-ndvi","title":"EDC Sentinel Hub: Data Fusion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-datasets","title":"EDC Sentinel Hub - using xcube to access different data sets in Sentinel Hub","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-setup","title":"Setup for EDC core API access using xcube client library","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-xcube-integration","title":"EDC Sentinel Hub - XCUBE integration","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-usecase-ndvi-timeline","title":"NDVI timeline","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.eo-learn","title":"Using eo-learn in EDC: a starter’s guide.","description":"","date":"","thumbnail":"/build/5a1f8475f0c3652f4630d03d72891b98.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.esdl-edc-v0-2","title":"Earth System Data Lab Tutorial @ Euro Data Cube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.exploring-time-and-space-with-edc","title":"Exploring Time and Space: A guide to accessing, analysing and visualising data in the Euro Data Cube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.how-to-access-dem-data-through-sentinel-hub-api","title":"How to access DEM data through Sentinel Hub API","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.sentinelhub-py","title":"Using Sentinel Hub Process API in EDC: a starter’s guide.","description":"","date":"","thumbnail":"/build/01e22903839391bc356ac886ec2b2fa7.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"title":"Notebooks","level":1},{"slug":"notebooks.example1","title":"📘 Example Notebook 1","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.example2","title":"📘 Example Notebook 2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"page":{"version":1,"kind":"Notebook","sha256":"d1fa2460b6ed455ce85a223fe2f4026cb70dc901f361da7277123c2ee28613c8","slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-notebook-lstm","location":"/external_notebooks/eurodatacube/notebooks/notebooks/contributions/edc_notebook_lstm.ipynb","dependencies":[],"frontmatter":{"title":"South Africa Crop Type Classification on Euro Data Cube (EDC)","content_includes_title":false,"kernelspec":{"name":"conda-env-users-edcg-2023.10-01-py","display_name":"users-edcg-2023.10-01","language":"python"},"github":"https://github.com/eoxhub-workspaces/documentation/","numbering":{"title":{"offset":5}},"edit_url":"https://github.com/eoxhub-workspaces/documentation//blob/main/external_notebooks/eurodatacube/notebooks/notebooks/contributions/edc_notebook_lstm.ipynb","exports":[{"format":"ipynb","filename":"edc_notebook_lstm.ipynb","url":"/build/edc_notebook_lstm-48b1179b10b44261bd19769ad25b0388.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This notebook shows the steps towards preparing data for training a supervised machine learning model on EDC. We will train the model based on the optical bands with 10 m resolution of Sentinel-2 imagery, i.e., the B2, B3, B4, and B8.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"DTuY5KWq07"}],"key":"RB8R2Ua3C5"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"We will use the Long Short-Term Memory model (LSTM), a variation of a recurrent neural network (RNN), which learns the temporal context of a particular yearly time series of a crop.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"FS1QqcRG7U"}],"key":"iRaWSNmz7d"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"As for the ground truth, we will use the ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"tEnZmXgRQ8"},{"type":"link","url":"https://collections.eurodatacube.com/south-africa-crops-competition/","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"South Africa Crop Type Competition","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"an5ocgq6D6"}],"urlSource":"https://collections.eurodatacube.com/south-africa-crops-competition/","key":"iSSZh9GfEF"},{"type":"text","value":" collection, which is part of the EDC public collection and contains the field identification label representing the area of crop fields and the corresponding crop type collected via aerial and vehicle surveys.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"S4aZQ9H8e6"}],"key":"ifR6erqX2k"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"In this example notebook, the expected outcome is a muticlass classifier that identifies different crop types planted in the fields in South Africa.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"adAgc1WwuF"}],"key":"qzyXIhHBND"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"We will prepare the training data in the following steps:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Y0voN727Gu"}],"key":"C5ayFrT1Tv"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Search for available ground truth labels","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"WzNIF5mKyH"}],"key":"p324jGc9nq"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Download features and labels","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"lcv937egK0"}],"key":"Qz05oS4vpB"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Reshape data for model training","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"yEJ9rAHAkp"}],"key":"SznjwDnAXq"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Normalize and undersample data","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"ffsFj8tvnd"}],"key":"EKAHWuNBNM"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Train model","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"HnuvLWRljB"}],"key":"JXhIjJsNEd"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Run model on validation data","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"t1XWDG7IcY"}],"key":"PJ2tMgvPdk"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Evaluate results","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"pTxGk5g7NB"}],"key":"fUq3pbJG1h"}],"key":"AJF3pDFi6B"}],"key":"YVZz8pRPvM"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"%load_ext autoreload\n%autoreload 2","visibility":"show","key":"ETADlejySG"},{"type":"output","id":"QB0AYAi9W2Rz0xzgNW5yD","data":[],"visibility":"show","key":"e5TGN9yo7t"}],"visibility":"show","key":"ucRSUXGgQ4"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import sys\n\nprint(sys.version)\n\n# NOTE: all necessary packages are preinstalled in the EuroDataCube curated 'edcg-2023.10-01' conda kernel!","visibility":"show","key":"AFWILOl2G8"},{"type":"output","id":"V2SvMYcr9QxR1ssUmJxaf","data":[{"name":"stdout","output_type":"stream","text":"3.10.13 | packaged by conda-forge | (main, Oct 26 2023, 18:07:37) [GCC 12.3.0]\n"}],"visibility":"show","key":"FpzopfHbSg"}],"visibility":"show","key":"F7y8lfMCfE"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import datetime\nimport os\nimport pickle\nfrom collections import defaultdict\n\nimport contextily as cx\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch\nimport torch.nn.functional as F\nfrom eolearn.core import (\n    EOExecutor,\n    EOPatch,\n    EOTask,\n    EOWorkflow,\n    FeatureType,\n    OverwritePermission,\n    SaveTask,\n    linearly_connect_tasks,\n)\nfrom eolearn.features.extra.interpolation import LinearInterpolationTask\nfrom eolearn.io import SentinelHubEvalscriptTask, SentinelHubInputTask\nfrom matplotlib.colors import BoundaryNorm, ListedColormap\nfrom sentinelhub import (\n    CRS,\n    Band,\n    BBox,\n    DataCollection,\n    SentinelHubStatistical,\n    SentinelHubStatisticalDownloadClient,\n    SHConfig,\n    Unit,\n    UtmZoneSplitter,\n    bbox_to_dimensions,\n)\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.auto import tqdm\n\nrng = np.random.default_rng(42)","visibility":"show","key":"BggMgtJOCC"},{"type":"output","id":"qI-hXnMcIMi4r6DP-a6m6","data":[],"visibility":"show","key":"sQ9ZG8glVG"}],"visibility":"show","key":"b4IG0xAmy7"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"1. Search for available ground truth’s labels","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HBKJ45q8r7"}],"identifier":"id-1-search-for-available-ground-truths-labels","label":"1. Search for available ground truth’s labels","html_id":"id-1-search-for-available-ground-truths-labels","implicit":true,"key":"VBvfpC2oMk"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"To start preparing a training dataset, we need to find the areas which contain ground truth available in the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"E7rAK5hgoC"},{"type":"link","url":"https://collections.eurodatacube.com/south-africa-crops-competition/","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"South Africa Crop Type Competition","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Kr73ZZbpXG"}],"urlSource":"https://collections.eurodatacube.com/south-africa-crops-competition/","key":"YPN5quazRt"},{"type":"text","value":" data collection on EDC. We will use the geographical coverage and the temporal availability provided on the linked webpage above to make a Catalog API request.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"IKKVaIHElw"}],"key":"atb2HfZgq7"}],"key":"CE1sNrQC8O"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# load the SH config\nconfig = SHConfig()\n\n# split the AOI into 2560m x 2560m tiles\nextent = BBox((17.85, -33.089240, 18.193359, -32.7), crs=CRS.WGS84)\nbbox_list = UtmZoneSplitter([extent.geometry], crs=extent.crs, bbox_size=[2560, 2560]).bbox_list","visibility":"show","key":"BMBk9ZPTPL"},{"type":"output","id":"lfKLV1nwWZiOHa1nbPzvI","data":[{"name":"stderr","output_type":"stream","text":"/home/conda/users/78c2e9a35d8f916b51e069b3359b976953e33c4ac102d2c530b5373dd8486c9d-20240215-082503-014013-382-edcg-2023.10-01/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n/home/conda/users/78c2e9a35d8f916b51e069b3359b976953e33c4ac102d2c530b5373dd8486c9d-20240215-082503-014013-382-edcg-2023.10-01/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n"}],"visibility":"show","key":"B9qTC7oO24"}],"visibility":"show","key":"MMjDNfbGEh"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"plt.style.use([\"default\", \"bmh\"])\nfig, ax = plt.subplots(figsize=(12, 5))\nfig.patch.set_alpha(1)\n\n# construct grid for plotting\ngrid = defaultdict(list)\nfor idx, bbox in enumerate(bbox_list):\n    grid[bbox.crs.epsg].append({\"geometry\": bbox.geometry, \"bbox_id\": idx})\n\ngdf_list = [\n    gpd.GeoDataFrame(subset, geometry=\"geometry\", crs=crs).to_crs(CRS.WGS84.epsg) for crs, subset in grid.items()\n]\ngdf = pd.concat(gdf_list).sort_values(\"bbox_id\").reset_index(drop=True)\n\n# plot grid and basemap\ngdf.plot(edgecolor=\"k\", alpha=0.2, ax=ax)\ncx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)","visibility":"show","key":"au6RhY8164"},{"type":"output","id":"87bbW_407iQlik2aUG9GW","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"e33aa4016a1f9b7fc3affc4a543094a6","path":"/build/e33aa4016a1f9b7fc3affc4a543094a6.png"},"text/plain":{"content":"\u003cFigure size 1200x500 with 1 Axes\u003e","content_type":"text/plain"}}}],"visibility":"show","key":"ea4gKgfRQq"}],"visibility":"show","key":"ZYG0SMtq7Y"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"def get_coverage_request(\n    bbox: BBox,\n    time_interval: tuple[str, str],\n    evalscript: str,\n    collection: DataCollection = DataCollection.SENTINEL2_L2A,\n):\n    \"\"\"Construct request for SentinelHubStatistical data to get coverage of labels in the tile\"\"\"\n    patch_size = bbox_to_dimensions(bbox, resolution=60)\n\n    return SentinelHubStatistical(\n        aggregation=SentinelHubStatistical.aggregation(\n            evalscript=evalscript,\n            time_interval=time_interval,\n            aggregation_interval=\"P1D\",\n            size=patch_size,\n        ),\n        input_data=[SentinelHubStatistical.input_data(collection)],\n        bbox=bbox,\n    )\n\n\n# prepare evalscript for downloading crop labels\nlabels_evalscript = \"\"\"\n//VERSION=3\n\nfunction setup() {\n    return {\n        input: [\n            {\"bands\": [\"crop\", \"dataMask\"]}\n        ],\n        output: [\n            {\n                id: \"LABELS\",\n                bands: 1,\n                sampleType: \"UINT8\"\n            },\n            {\n                id: \"dataMask\",\n                bands: 1,\n                sampleType: \"UINT8\"\n            }\n        ]\n    };\n}\n\nfunction evaluatePixel(sample) {\n    return {LABELS: [sample.crop], dataMask: [sample.dataMask]};\n}\n\"\"\"\n\n# define collection which points to crop labels\ncollection_id = \"bd457670-af1b-45cd-bef2-6a7c93bf5e6e\"\nsouth_africa_crop = DataCollection.define_byoc(\n    collection_id, bands=[Band(name=\"crop\", units=(Unit.DN,), output_types=(np.uint8,))], metabands=[], is_timeless=True\n)\n\n# specify labels definition time (from webpage) and run the requests\nlabels_time_interval = (\"2017-08-01\", \"2017-08-02\")\nkwargs = dict(time_interval=labels_time_interval, evalscript=labels_evalscript, collection=south_africa_crop)\nrequests = [get_coverage_request(bbox, **kwargs).download_list[0] for bbox in bbox_list]\nclient = SentinelHubStatisticalDownloadClient()\n\n# download or load data from disk\nif not os.path.exists(\"stats.pkl\"):\n    stats = client.download(requests)\n    pickle.dump(stats, open(\"stats.pkl\", \"wb\"))\nelse:\n    stats = pickle.load(open(\"stats.pkl\", \"rb\"))","visibility":"show","key":"LOWMBwJ3Wu"},{"type":"output","id":"82utOBh0w6OEYU8JqNl_2","data":[],"visibility":"show","key":"MpcUxHvCj2"}],"visibility":"show","key":"xU1N56krtb"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"def extract_coverage(stats: list[dict]):\n    \"\"\"Extract labels coverage from the statistical data\"\"\"\n    data = stats[\"data\"][0][\"outputs\"][\"LABELS\"][\"bands\"][\"B0\"][\"stats\"]\n    return 1 - data[\"noDataCount\"] / data[\"sampleCount\"]\n\n\n# extract coverage from the stats\ncoverages = np.array([extract_coverage(s) if s[\"data\"] else 0 for s in stats])","visibility":"show","key":"ywMke2Gvuq"},{"type":"output","id":"hvsXNw4h0OSjRJqkepCjk","data":[],"visibility":"show","key":"cOBLcGJixf"}],"visibility":"show","key":"uNEjUQZERY"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"plt.style.use([\"default\", \"bmh\"])\nfig, ax = plt.subplots(figsize=(15, 5))\nfig.patch.set_alpha(1)\n\n# plot labels coverage over tiles, ignore tiles with no data\nax.hist(coverages[coverages \u003e 0], bins=30)\nax.set_xlabel(\"Reference data coverage\");","visibility":"show","key":"fgmhxGBELL"},{"type":"output","id":"2t72REfA--XmcZgx08w5V","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"9ca6b536533a2d663a7a1012cdad50b0","path":"/build/9ca6b536533a2d663a7a1012cdad50b0.png"},"text/plain":{"content":"\u003cFigure size 1500x500 with 1 Axes\u003e","content_type":"text/plain"}}}],"visibility":"show","key":"E00D0mteDo"}],"visibility":"show","key":"tfefrxzhzF"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"","key":"mYEES5HHDh"},{"type":"output","id":"8Lcl90lmFbGGp3cUn__4g","data":[],"key":"OddmW1viBv"}],"key":"tpu1j4lSJG"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# let's keep only the tiles with coverage above 40%\ncoverage_filter = np.array(coverages) \u003e 0.40\nprint(f\"{np.count_nonzero(coverage_filter)} eligible patches\")\n\nfiltered = np.array(bbox_list)[coverage_filter]","visibility":"show","key":"mA5s5cyNHl"},{"type":"output","id":"btO-bAdFZOHDOBxSsD9c0","data":[{"name":"stdout","output_type":"stream","text":"63 eligible patches\n"}],"visibility":"show","key":"lMb8FQ9avA"}],"visibility":"show","key":"LnzdjwaWjf"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"plt.style.use([\"default\", \"bmh\"])\nfig, ax = plt.subplots(figsize=(12, 5))\nfig.patch.set_alpha(1)\n\n# set new column\ngdf[\"eligible\"] = coverage_filter\n\n# plot grid and basemap\ngdf.plot(color=\"none\", edgecolor=\"k\", alpha=0.2, ax=ax)\ngdf[gdf[\"eligible\"]].plot(color=\"C0\", edgecolor=\"k\", alpha=0.5, ax=ax)\ncx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)","visibility":"show","key":"PbZKob9ImC"},{"type":"output","id":"e1lBT6Phiu525oGfRNgXd","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"692056e1100b97481d8ae22cda7d69a3","path":"/build/692056e1100b97481d8ae22cda7d69a3.png"},"text/plain":{"content":"\u003cFigure size 1200x500 with 1 Axes\u003e","content_type":"text/plain"}}}],"visibility":"show","key":"hyFkCFLw2w"}],"visibility":"show","key":"cBGa99Z2rL"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"2. Download features and labels","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PbmTQRgGel"}],"identifier":"id-2-download-features-and-labels","label":"2. Download features and labels","html_id":"id-2-download-features-and-labels","implicit":true,"key":"oJ1iwbZGTj"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now we know where the groud truth is available, we use eo-learn to download the data and the labels. This is what the eo-learn workflow does:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"qDXENWBvpd"}],"key":"pBKtaIspEU"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Download bands data (B, G, R, NIR)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"IB5nC9RU8W"}],"key":"DSqEob63or"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Download labels data","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"iLCbumQzI0"}],"key":"QQbVCVdr31"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Construct valid mask from data availability mask and cloud mask","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"yfz7YYoDa5"}],"key":"aQTUQjZoJd"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Perform temporal interpolation and resample to uniform timestamps across whole AOI","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"VJSxJqafWc"}],"key":"fVQ6bBJY9U"}],"key":"suJp9b6Go8"}],"key":"sw9f9flAZf"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For the purpose of this notebook, we will download only 2 EOPatches, but to properly train the model, set the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZulABOMLwy"},{"type":"inlineCode","value":"tutorial_mode","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cJHj1HaMbT"},{"type":"text","value":" to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dpMelrengF"},{"type":"inlineCode","value":"False","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kjabrophWZ"},{"type":"text","value":" to properly train the model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EF1aIgAdj8"}],"key":"S5MCKReba2"}],"key":"XcePpnr93X"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"tutorial_mode = True\nif tutorial_mode:  # select first two eligible EOPatches\n    subset = [18, 20]\n    selected = filtered[subset]\n    gdf[\"selected\"] = False\n    gdf.loc[gdf.loc[gdf.eligible].index[subset], \"selected\"] = True\nelse:  # select all eligible EOPatches\n    selected = filtered\n    gdf[\"selected\"] = coverage_filter","visibility":"show","key":"MOVRUBqtME"},{"type":"output","id":"YEb50fDMXdk6hHVYXCTy2","data":[],"visibility":"show","key":"QMlZhDDi3r"}],"visibility":"show","key":"yzdH6nSQUI"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# prepare task for joining valid data mask and cloud masks from SH\nclass SentinelHubValidDataTask(EOTask):\n    \"\"\"\n    Combine the downloaded cloud mask with `IS_DATA` to define a valid data mask\n    The SentinelHub's cloud mask is expected in eopatch.mask['CLM']\n    \"\"\"\n\n    def __init__(self, output_feature):\n        self.output_feature = output_feature\n\n    def execute(self, eopatch):\n        eopatch[self.output_feature] = eopatch.mask[\"IS_DATA\"].astype(bool) \u0026 (~eopatch.mask[\"CLM\"].astype(bool))\n        return eopatch\n\n\n# BAND DATA\n# Add a request for S2 bands.\n# Here we also do a simple filter of cloudy scenes (on tile level).\n# The s2cloudless masks are requested via additional data.\nband_names = [\"B02\", \"B03\", \"B04\", \"B08\"]\nadd_data = SentinelHubInputTask(\n    bands_feature=(FeatureType.DATA, \"BANDS\"),\n    bands=band_names,\n    resolution=10,\n    maxcc=0.8,\n    time_difference=datetime.timedelta(minutes=120),\n    data_collection=DataCollection.SENTINEL2_L2A,\n    additional_data=[(FeatureType.MASK, \"dataMask\", \"IS_DATA\"), (FeatureType.MASK, \"CLM\")],\n    max_threads=5,\n)\n\n# LABELS DATA\n# Add a request for crop labels.\nadd_labels = SentinelHubEvalscriptTask(\n    features=(FeatureType.MASK_TIMELESS, \"LABELS\"),\n    evalscript=labels_evalscript,\n    data_collection=south_africa_crop,\n    resolution=10,\n    max_threads=5,\n)\n\n# VALIDITY MASK\n# Validate pixels using SentinelHub's cloud detection mask and region of acquisition\nadd_sh_validmask = SentinelHubValidDataTask((FeatureType.MASK, \"IS_VALID\"))\n\n# LINEAR TEMPORAL INTERPOLATION\n# linear interpolation of full time-series and date resampling\n# needed to evaluate time series at specific dates for all data\ntime_interval = (\"2017-01-01\", \"2017-12-31\")\nresampled_range = (time_interval[0], time_interval[1], 15)  # define target timestamps (every 15 days)\nlinear_interp = LinearInterpolationTask(\n    (FeatureType.DATA, \"BANDS\"),  # name of field to interpolate\n    mask_feature=(FeatureType.MASK, \"IS_VALID\"),  # mask to be used in interpolation\n    copy_features=[(FeatureType.MASK_TIMELESS, \"LABELS\")],  # features to keep\n    resample_range=resampled_range,\n)\n\n# SAVING TO OUTPUT\nsave = SaveTask(\"./eopatches\", overwrite_permission=OverwritePermission.OVERWRITE_FEATURES)","visibility":"show","key":"svCGPBvTDq"},{"type":"output","id":"tHSdQVrqztZv76PtzDgVl","data":[],"visibility":"show","key":"Wrevgj7MME"}],"visibility":"show","key":"mYlWz4Cdy6"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# Define the workflow\nworkflow_nodes = linearly_connect_tasks(\n    add_data,\n    add_labels,\n    add_sh_validmask,\n    linear_interp,\n    save,\n)\nworkflow = EOWorkflow(workflow_nodes)\n\n# Define additional parameters of the workflow\ninput_bands = workflow_nodes[0]\nsave_node = workflow_nodes[-1]\nexecution_args = []\nfor idx, bbox in enumerate(selected):\n    execution_args.append(\n        {\n            input_bands: {\"bbox\": bbox, \"time_interval\": time_interval},\n            save_node: {\"eopatch_folder\": f\"eopatch_{idx}\"},\n        }\n    )\n\n# Execute the workflow\nif not os.path.exists(\"eopatches\"):\n    executor = EOExecutor(workflow, execution_args, save_logs=True)\n    executor.run(workers=2)","visibility":"show","key":"MQVZBoUIPf"},{"type":"output","id":"I_rFH4vwSmxGT4CTH2oIk","data":[{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"5224204b0f364154932cacb96d19746f\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"text/plain":{"content":"  0%|          | 0/2 [00:00\u003c?, ?it/s]","content_type":"text/plain"}}}],"visibility":"show","key":"lo3Q6ZHbXJ"}],"visibility":"show","key":"g4naKqHYm5"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# collect the data and labels over all eopatches\ndata_list = []\nlabels_list = []\nfor eop_idx in tqdm(range(len(selected))):\n    eop = EOPatch.load(f\"./eopatches/eopatch_{eop_idx}\")\n    data_list.append(eop.data[\"BANDS\"])\n    labels_list.append(eop.mask_timeless[\"LABELS\"])","visibility":"show","key":"gI10KOHDZq"},{"type":"output","id":"U5j29bPI_z9UWghwwkVWj","data":[],"visibility":"show","key":"eQK12cp9og"}],"visibility":"show","key":"lnbpXqoF0c"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# plot selected scenes in true color\nncols = 2\nnrows = 1\ntime_idx = 12\n\nplt.style.use([\"default\", \"bmh\"])\nfig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 2, nrows * 2))\nfig.patch.set_alpha(1)\n\nfor ax, image in zip(axs.flatten(), data_list):\n    ax.imshow(np.clip(image[time_idx][..., [2, 1, 0]] * 3.5, 0, 1))\n    ax.axis(\"off\")\n\nplt.tight_layout()","key":"GeGHb40VaG"},{"type":"output","id":"MXBnwGZLsPJrIQtFGW2Wd","data":[],"key":"noC7RKTVBx"}],"key":"bRdodOUE3Z"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# plot also labels\nncols = 2\nnrows = 1\n\nlabel_codes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nlabel_colors = [\n    \"#ffffff\",\n    \"#a6cee3\",\n    \"#1f78b4\",\n    \"#b2df8a\",\n    \"#33a02c\",\n    \"#fb9a99\",\n    \"#e31a1c\",\n    \"#fdbf6f\",\n    \"#ff7f00\",\n    \"#cab2d6\",\n]\nlabel_text = [\n    \"No data\",\n    \"Lucerne/Medics\",\n    \"Planted pastures (perennial)\",\n    \"Fallow\",\n    \"Wine grapes\",\n    \"Weeds\",\n    \"Small grain grazing\",\n    \"Wheat\",\n    \"Canola\",\n    \"Rooibos\",\n]\n\nplt.style.use([\"default\", \"bmh\"])\nfig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 2, nrows * 2))\nfig.patch.set_alpha(1)\n\ncmap = ListedColormap(label_colors)\nnorm = BoundaryNorm(label_codes, 10)\n\nfor ax, image in zip(axs.flatten(), labels_list):\n    ax.imshow(image.squeeze(-1), cmap=cmap, norm=norm, interpolation=\"none\")\n    ax.axis(\"off\")\n\nplt.tight_layout()","key":"ezTKe2GML2"},{"type":"output","id":"GBiU6GamgoFglG4A_cP8F","data":[],"key":"snTjLUYldO"}],"key":"TVOo8cia4v"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"3. Reshape data for model training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ql2OBsL6BL"}],"identifier":"id-3-reshape-data-for-model-training","label":"3. Reshape data for model training","html_id":"id-3-reshape-data-for-model-training","implicit":true,"key":"QnQAb2b0Pi"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The data for the LSTM model needs to be in the form of ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Li3v6HmA1A"},{"type":"inlineCode","value":"N x T x C","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"gS5VfY4kX0"},{"type":"text","value":", where ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"GyQhUWpBvy"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"N","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"U9RarJ54YR"}],"key":"nqEJN5YNWG"},{"type":"text","value":" represents the number of pixels, ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"NQIWoIwl54"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"T","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"qYaEbCB1Ps"}],"key":"kM5ytBJqgH"},{"type":"text","value":" represents the number of timestamps, and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HDJXyLVzGF"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"C","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"X6LUnKZ6mj"}],"key":"FzW2Ae6gF1"},{"type":"text","value":" the number of channels.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"FHYh0H5PGS"}],"key":"KFfd0lG9ym"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"We have 25 timestamps and 4 channels (B, G, R, NIR).","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"JHq4GdMyxJ"}],"key":"buVMppx82H"}],"key":"gkwmGkWObj"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# split tiles into train and validation patches\ntrain_slice = slice(None, None, 2)  # evens\nval_slice = slice(1, None, 2)  # odds\n\ntrain_data, train_labels = data_list[train_slice], labels_list[train_slice]\nvalidate_data, validate_labels = data_list[val_slice], labels_list[val_slice]\n\nsel_gdf = gdf[gdf.selected].copy()\nsel_gdf[\"train_split\"] = False\nsel_gdf.loc[train_slice, \"train_split\"] = True\n\nplt.style.use([\"default\", \"bmh\"])\nfig, ax = plt.subplots(figsize=(12, 5))\nfig.patch.set_alpha(1)\n\n# plot grid and basemap\ngdf.plot(color=\"none\", edgecolor=\"k\", alpha=0.2, ax=ax)\nsel_gdf[sel_gdf.train_split].plot(color=\"xkcd:green\", edgecolor=\"k\", alpha=0.2, ax=ax)\nsel_gdf[~sel_gdf.train_split].plot(color=\"xkcd:brick\", edgecolor=\"k\", alpha=0.2, ax=ax)\ncx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)","key":"BP3LIhnwjT"},{"type":"output","id":"lSsA9tlHVefexYYEe9ML0","data":[],"key":"wfqja0P39x"}],"key":"ZWW3Mm8cxj"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# function for extracting and reshaping the data and labels\ndef get_model_input(data: np.ndarray, labels: np.ndarray) -\u003e tuple:\n    t, h, w, d = data.shape\n    x = np.reshape(np.moveaxis(data, 0, -2), (h * w, t, d))  # N x T x C\n    y = np.reshape(labels, (h * w))\n    return x[y != 0], y[y != 0]","key":"DIf3UmAZRb"},{"type":"output","id":"U3nysOptIO-WA-mLBCLo-","data":[],"key":"S3qcV50FsG"}],"key":"jCbiKkiWc5"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# reshape for train data\nx_train, y_train = [], []\nfor data, labels in zip(train_data, train_labels):\n    model_input = get_model_input(data, labels)\n    x_train.append(model_input[0])\n    y_train.append(model_input[1])\n\nx_train = np.concatenate(x_train, axis=0).astype(np.float32)\ny_train = np.concatenate(y_train, axis=0).astype(np.uint8)\ndel train_data, train_labels\n\nprint(x_train.shape)\nprint(y_train.shape)","key":"Lv7Z3SvKsk"},{"type":"output","id":"qv1omFrWtht8A6fCVO_Bl","data":[],"key":"Pr0fiJLiBB"}],"key":"eYbZmyJV2Y"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# reshape for validation data\nx_validate, y_validate = [], []\nfor data, labels in zip(validate_data, validate_labels):\n    model_input = get_model_input(data, labels)\n    x_validate.append(model_input[0])\n    y_validate.append(model_input[1])\n\nx_validate = np.concatenate(x_validate, axis=0).astype(np.float32)\ny_validate = np.concatenate(y_validate, axis=0).astype(np.uint8)\ndel validate_data, validate_labels\n\nprint(x_validate.shape)\nprint(y_validate.shape)","key":"EtkJQv2eWP"},{"type":"output","id":"1Z73lEpcMzrffoIUNe02r","data":[],"key":"j3FRllgjjB"}],"key":"cqBowsjSti"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"plt.style.use([\"default\", \"bmh\"])\nfig, ax = plt.subplots(figsize=(15, 5), ncols=2, sharex=True)\nfig.patch.set_alpha(1)\n\nlabels_dict = dict(zip(label_codes, label_text))\n\nlabels, counts = np.unique(y_train, return_counts=True)\nax[0].barh(range(len(labels)), counts)\nax[0].set_title(\"Train data labels distribution\")\nax[0].set_yticks(range(len(labels)), [labels_dict[lbl] for lbl in labels])\n\nlabels, counts = np.unique(y_validate, return_counts=True)\nax[1].barh(range(len(labels)), counts)\nax[1].set_title(\"Validation data labels distribution\")\nax[1].set_yticks(range(len(labels)), [labels_dict[lbl] for lbl in labels])\n\nplt.tight_layout()","key":"ssHRZ4w4Ge"},{"type":"output","id":"bMWLdtXrTL3oIDU4U0lRp","data":[],"key":"NeWurqdt5C"}],"key":"XbzhsxjH4Z"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"4. Normalize and undersample data","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XwgpDnc0nL"}],"identifier":"id-4-normalize-and-undersample-data","label":"4. Normalize and undersample data","html_id":"id-4-normalize-and-undersample-data","implicit":true,"key":"A4u5iXYt9F"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"For the columns that contain numerical data, we should ensure the data range is normalized in case the data has different scales. This process may improve the models performance, and also preserve outliers.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"WZYfa0jjiJ"}],"key":"NxhDl2PrfD"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Here we will use a simple method of undersampling just to minimize the amount of huge data we are handling, but the balance of the classes will remain the same.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Vav55O4a6d"}],"key":"B3QSbKzv2H"}],"key":"EFy3P9V4YY"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# set random seet\nnp.random.seed(42)\n\n# define percentage of data to use\npercentage = 0.3\n\n# sample train data\nsample_mask = np.random.choice(x_train.shape[0], int(percentage * x_train.shape[0]))\nclass_counts_before = np.unique(y_train, return_counts=True)\nx_train = x_train[np.sort(sample_mask)]\ny_train = y_train[np.sort(sample_mask)]\nclass_counts_after = np.unique(y_train, return_counts=True)\n\n# sample validation data\nsample_mask_val = np.random.choice(x_validate.shape[0], int(percentage * x_validate.shape[0]))\nx_validate = x_validate[sample_mask_val]\ny_validate = y_validate[sample_mask_val]\n\n# normalize train data\nn, t, c = x_train.shape\nx_train = StandardScaler().fit_transform(x_train.reshape(n * t, c)).reshape(n, t, c)\n\n# normalize validation data\nn, t, c = x_validate.shape\nval_scaler = StandardScaler().fit(x_validate.reshape(n * t, c))\nx_validate = val_scaler.transform(x_validate.reshape(n * t, c)).reshape(n, t, c)\n\n# check shapes\nx_train.shape, x_validate.shape","key":"iOs4lQXs3k"},{"type":"output","id":"AFvWvs8QZcSokwIMVzeGe","data":[],"key":"C5eYIpSlzC"}],"key":"TYfpWBHNYZ"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# fill nan values with -1\nx_train = np.nan_to_num(x_train, nan=-1)\nx_validate = np.nan_to_num(x_validate, nan=-1)","key":"FyIV0Yv1AG"},{"type":"output","id":"CGvL8wZrJ4dJoO4FGkHmB","data":[],"key":"Fcr6PAQS80"}],"key":"DGpz7xaRKr"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# check labels distribution before and after undersampling\nplt.style.use([\"default\", \"bmh\"])\nfig, axs = plt.subplots(figsize=(12, 7), ncols=2)\nfig.patch.set_alpha(1)\n\nlabels_dict = dict(zip(label_codes, label_text))\ncolors_dict = dict(zip(label_codes, label_colors))\n\naxs[0].pie(\n    class_counts_before[1],\n    labels=[labels_dict[lbl] for lbl in class_counts_before[0]],\n    autopct=\"%1.1f%%\",\n    colors=[colors_dict[lbl] for lbl in class_counts_before[0]],\n    wedgeprops={\"linewidth\": 2, \"edgecolor\": \"white\", \"alpha\": 0.7},\n)\naxs[0].set_title(\"Class Distribution Before Undersampling\")\n\naxs[1].pie(\n    class_counts_after[1],\n    labels=[labels_dict[lbl] for lbl in class_counts_after[0]],\n    autopct=\"%1.1f%%\",\n    colors=[colors_dict[lbl] for lbl in class_counts_after[0]],\n    wedgeprops={\"linewidth\": 2, \"edgecolor\": \"white\", \"alpha\": 0.7},\n)\naxs[1].set_title(\"Class Distribution After Undersampling\")\n\nplt.tight_layout()","key":"CxHQXLtTVs"},{"type":"output","id":"-OqP5BoD857rxWDHeyaSW","data":[],"key":"LcInEONn5R"}],"key":"hI3JTEJTj0"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"5. Train the model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZFRmUzT2OK"}],"identifier":"id-5-train-the-model","label":"5. Train the model","html_id":"id-5-train-the-model","implicit":true,"key":"KabPMUmEH5"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now it’s time to construct and train the LSTM model. First we need a bunch of classes that the model will use, from the dataloaders to the model itself.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"JaZL7XZbnF"}],"key":"H6TWlOgUej"}],"key":"gkB0d4o5n8"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Define the data loaders","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wGKBqV2Svg"}],"identifier":"define-the-data-loaders","label":"Define the data loaders","html_id":"define-the-data-loaders","implicit":true,"key":"HZCSUNkmIC"}],"key":"czP4KYYUbu"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# class for the dataloader\nclass CustomImageDataset(Dataset):\n    def __init__(self, x_data: np.ndarray, y_data: np.ndarray, encoder: LabelEncoder):\n        self.x_data = x_data\n        self.y_data = encoder.transform(y_data)\n\n    def __len__(self):\n        return len(self.y_data)\n\n    def __getitem__(self, idx):\n        return self.x_data[idx], self.y_data[idx]\n\n\n# encode labels to 0 - num_labels\nencoder = LabelEncoder().fit(list(set(y_train) | set(y_validate)))\nfor lbl in encoder.classes_:\n    print(f\"{lbl} -\u003e {encoder.transform([lbl])[0]}\")","key":"a0hWhM13Wa"},{"type":"output","id":"pFvdR86WT9ERrTljCGtX7","data":[],"key":"AIQ4adzhXV"}],"key":"LukNKMPBtZ"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# define the batch size and the dataloaders for train/validation data\nBATCH_SIZE = 1024 if tutorial_mode else 4096\ntrain_dataloader = DataLoader(CustomImageDataset(x_train, y_train, encoder), batch_size=BATCH_SIZE, shuffle=True)\nvalidation_dataloader = DataLoader(\n    CustomImageDataset(x_validate, y_validate, encoder), batch_size=BATCH_SIZE, shuffle=True\n)","key":"jkwDoY0qcw"},{"type":"output","id":"gIKY9RuGNed97osRf_lEn","data":[],"key":"TntwsBDjrv"}],"key":"iphdLKY8Vx"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Define the model architecture","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZUQ6Auusvy"}],"identifier":"define-the-model-architecture","label":"Define the model architecture","html_id":"define-the-model-architecture","implicit":true,"key":"DWfbmfju0B"}],"key":"y3ES0RMYTk"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"class AverageMetric:\n    \"\"\"\n    Simple class for averaging metrics.\n    \"\"\"\n\n    def __init__(self):\n        self.values = list()\n\n    def add(self, new):\n        self.values.append(new)\n\n    def get(self):\n        return np.array(self.values).mean()\n\n\n# This code is taken from https://github.com/TUM-LMF/BreizhCrops\nclass LSTM(nn.Module):\n    \"\"\"\n    Implementation of the LSTM model for classification of a input sequence into n classes.\n\n    :param input_dim: number of input features entering LSTM cell (i.e. 13 if all S2 bands are used)\n    :type input_dim: int\n    :param n_classes: number of classes\n    :type n_classes: int (class labels have to be between 0 and n_classes-1)\n    :param hidden_dims: The number of features in the hidden state `h`\n    :type hidden_dims: int\n    :param num_rnn_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n                           would mean stacking two LSTMs together to form a `stacked LSTM`,\n                           with the second LSTM taking in outputs of the first LSTM and\n                           computing the final results.\n    :type num_rnn_layers: int\n    :param dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n                    LSTM layer except the last layer, with dropout probability equal to\n                    :attr:`dropout`.\n    :type dropout: float (between 0.0 and 1.0)\n    :param bidirectional: If ``True``, becomes a bidirectional LSTM.\n    :type bidirectional: bool\n    :param use_batchnorm: If ``True``, use batch normalization.\n    :type use_batchnorm: bool\n    :param use_layernorm: If ``True``, applies Layer Normalization over a mini-batch of inputs.\n    :type use_layernorm: bool\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim,\n        n_classes,\n        hidden_dims,\n        num_rnn_layers=1,\n        dropout=0,\n        bidirectional=False,\n        use_batchnorm=False,\n        use_layernorm=True,\n    ):\n        super().__init__()\n\n        self.nclasses = n_classes\n        self.use_batchnorm = use_batchnorm\n        self.use_layernorm = use_layernorm\n\n        self.d_model = num_rnn_layers * hidden_dims\n        if use_layernorm:\n            self.inlayernorm = nn.LayerNorm(input_dim)\n            self.clayernorm = nn.LayerNorm((hidden_dims + hidden_dims * bidirectional) * num_rnn_layers)\n\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dims,\n            num_layers=num_rnn_layers,\n            bias=False,\n            batch_first=True,\n            dropout=dropout,\n            bidirectional=bidirectional,\n        )\n\n        if bidirectional:\n            hidden_dims = hidden_dims * 2\n\n        self.linear_class = nn.Linear(hidden_dims * num_rnn_layers, n_classes, bias=True)\n\n        if use_batchnorm:\n            self.bn = nn.BatchNorm1d(hidden_dims)\n\n    def _logits(self, x):\n        x = x.transpose(1, 2)\n\n        if self.use_layernorm:\n            x = self.inlayernorm(x)\n\n        outputs, last_state_list = self.lstm.forward(x)\n\n        # TODO: check what is goinig on here\n        if self.use_batchnorm:\n            b, t, d = outputs.shape\n            o_ = outputs.view(b, -1, d).permute(0, 2, 1)\n            outputs = self.bn(o_).permute(0, 2, 1).view(b, t, d)\n\n        h, c = last_state_list\n\n        nlayers, batchsize, n_hidden = c.shape\n        # TODO: shouldn't this be executed only uf layernorm is True\n        h = self.clayernorm(c.transpose(0, 1).contiguous().view(batchsize, nlayers * n_hidden))\n        logits = self.linear_class.forward(h)\n\n        return logits\n\n    def forward(self, x):\n        logits = self._logits(x)\n        logprobabilities = F.log_softmax(logits, dim=-1)\n        return logprobabilities\n\n    def save(self, path=\"model.pth\", **kwargs):\n        model_state = self.state_dict()\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        torch.save(dict(model_state=model_state, **kwargs), path)\n\n    def load(self, path):\n        snapshot = torch.load(path, map_location=\"cpu\")\n        model_state = snapshot.pop(\"model_state\", snapshot)\n        self.load_state_dict(model_state)\n        return snapshot\n\n    def update_and_freeze_body(self, pretrained, freeze_layers=(0, -1)):\n        self.inlayernorm = pretrained.inlayernorm\n        self.clayernorm = pretrained.clayernorm\n        self.lstm = pretrained.lstm\n\n        for child in list(self.children())[freeze_layers[0] : freeze_layers[1]]:\n            for param in child.parameters():\n                param.requires_grad = False\n\n    def unfreeze(self):\n        for child in self.children():\n            for param in child.parameters():\n                param.requires_grad = True\n\n\ndef train(model, optimizer, train_loader, validation_loader, epochs):\n    \"\"\"\n    Vanilla function to run the inference on all train samples given in training dataloader.\n    At each epoch end the loss and accuracy of samples from the validation loader are printed.\n    \"\"\"\n    model.train()\n\n    if torch.cuda.is_available():\n        model = model.cuda()\n\n    for epoch in range(epochs):\n        # train phase\n        train_loss_log = AverageMetric()\n        model.train()\n        for data in train_loader:\n            optimizer.zero_grad()\n\n            inputs, targets = data\n\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                targets = targets.cuda()\n\n            logprobabilities = model.forward(inputs.transpose(1, 2))\n\n            loss = torch.nn.functional.nll_loss(logprobabilities, targets)\n            train_loss_log.add(loss.cpu().detach().numpy())\n\n            loss.backward()\n            optimizer.step()\n\n        # val phase\n        val_loss_log = AverageMetric()\n        model.eval()\n        predictions_list = list()\n        targets_list = list()\n        for _, data in enumerate(validation_loader):\n            inputs, targets = data\n\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                targets = targets.cuda()\n\n            logprobabilities = model.forward(inputs.transpose(1, 2))\n            loss = torch.nn.functional.nll_loss(logprobabilities, targets)\n            val_loss_log.add(loss.cpu().detach().numpy())\n\n            targets_list.append(targets.cpu().detach().numpy())\n            predictions_list.append((logprobabilities.cpu().detach().numpy()).argmax(1))\n\n        val_acc = accuracy_score(np.concatenate(targets_list), np.concatenate(predictions_list))\n        print(\n            f\"Epoch {epoch}: train loss {train_loss_log.get():.3f} | \"\n            f\"val loss {val_loss_log.get():.3f} | \"\n            f\"val acc = {val_acc:.3f}\"\n        )\n\n    return model","key":"GNtWEsIlUY"},{"type":"output","id":"vpYzymOsJTY7-zvG93HF-","data":[],"key":"FIrhIY67On"}],"key":"T5XkMx9cqw"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# set up the model\nmodel = LSTM(\n    input_dim=x_train.shape[-1],\n    n_classes=len(encoder.classes_),\n    hidden_dims=128,\n    num_rnn_layers=3,\n    dropout=0.1,\n    bidirectional=True,\n    use_batchnorm=False,\n    use_layernorm=True,\n)\n\n# set up the optimizer\noptimizer = Adam(model.parameters(), lr=5e-5)","key":"wZ26AZJYVZ"},{"type":"output","id":"F2NMNpxV-ed9ZeebtlS4q","data":[],"key":"GeIkoEVrvP"}],"key":"Yun0zr18TQ"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"model_path = \"./model.pth\"\nif not os.path.exists(model_path):\n    n_epochs = 2 if tutorial_mode else 50\n    # train the model for a few epochs and save when training is done\n    model = train(model, optimizer, train_dataloader, validation_dataloader, epochs=n_epochs)\n    model.save(model_path)\nelse:\n    # or load the saved model from path\n    model = LSTM(\n        input_dim=x_train.shape[-1],\n        n_classes=len(encoder.classes_),\n        hidden_dims=128,\n        num_rnn_layers=3,\n        dropout=0.1,\n        bidirectional=True,\n        use_batchnorm=False,\n        use_layernorm=True,\n    )\n    model.load(model_path)\n    model.eval()","key":"VSQkNHfFDK"},{"type":"output","id":"MIhMkaoLIJnsYbwpuaJYc","data":[],"key":"AHilfr2DZh"}],"key":"AOmILQdelf"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"6. Run the model on validation data","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gtaVJAl9YT"}],"identifier":"id-6-run-the-model-on-validation-data","label":"6. Run the model on validation data","html_id":"id-6-run-the-model-on-validation-data","implicit":true,"key":"qEUNWUAKYO"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now that the model is trained, let’s run it on the validation data and see how well it performs.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aURwTGO8kP"}],"key":"t7vF0so1Px"}],"key":"DzmDXBHcnF"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# convert validation data to the proper format for inference\nn_samples = 2000 if tutorial_mode else 10000\nval_sample_mask = np.random.choice(range(len(x_validate)), n_samples)\nval_tensor = torch.from_numpy(x_validate[val_sample_mask]).transpose(1, 2)\nlogprobas = model.forward(val_tensor).cpu().detach().numpy()","key":"iPcMJdSgxp"},{"type":"output","id":"hFbMw2x2LTEH7Sw8k-o4v","data":[],"key":"dBJZ8AV8J7"}],"key":"oDink2ETQ6"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# calculate the confidence and predictions\nprobas = np.exp(logprobas)\npredictions = encoder.classes_[probas.argmax(axis=1)]","key":"KgDCjyUGqt"},{"type":"output","id":"_kVALj6KuyLvJbrGhxv6F","data":[],"key":"gsddiHoY63"}],"key":"xhLZ7JXnx0"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# Calculate Accuracy, Precision and Recall, F1-Score\ny_true = y_validate[val_sample_mask]\nlabels = sorted(set(y_true) | set(predictions))\n\naccuracy = accuracy_score(y_true, predictions)\nprecision = precision_score(y_true, predictions, average=None, labels=labels)\nrecall = recall_score(y_true, predictions, average=None, labels=labels)\nf1 = f1_score(y_true, predictions, average=None, labels=labels)","key":"XjzSERGnY0"},{"type":"output","id":"bpWeLAD522afat6v-FNgt","data":[],"key":"gNByOpIgtM"}],"key":"obLTfuYJbD"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Plot F1-score per label","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KMOzZyWQcI"}],"key":"olUVVQVlgC"}],"key":"IM7I9DwWyb"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"fig, ax = plt.subplots(figsize=(12, 5))\n\n# calculate F1 score dataframe\ndf_f1 = pd.DataFrame({\"Code\": lbl, \"Label\": [labels_dict[lbl] for lbl in labels], \"F1 score\": f1})\n\n# plot scores\nax.barh(df_f1[\"Label\"], df_f1[\"F1 score\"])\n\nax.set_title(\"Labels F1-Score\")\nax.set_xlabel(\"F1-Score\")","key":"ckM6g8W4Xn"},{"type":"output","id":"JrYnT-VBh7PyHIuEchCQ9","data":[],"key":"e2FqHh8dmJ"}],"key":"ylE316aOKo"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Plot confusion matrix","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"clAEXUcfBT"}],"key":"v7WaWZiO6S"}],"key":"Z6Ih6anNAQ"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"true_labels_counts_dict = dict(zip(*np.unique(y_validate[val_sample_mask].tolist(), return_counts=True)))\ncounts = [true_labels_counts_dict.get(lbl, 0) for lbl in labels]\n\nlabels = np.array(labels)[np.argsort(counts)[::-1]]\ncounts = sorted(counts)[::-1]\n\ncm = confusion_matrix(y_true, predictions, normalize=\"true\", labels=labels)\nax = sns.heatmap(\n    cm,\n    annot=True,\n    cmap=\"Greens\",\n    fmt=\".2f\",\n    xticklabels=[labels_dict[lbl] for lbl in labels],\n    yticklabels=[f\"{labels_dict[lbl]}\\n({c})\" for lbl, c in zip(labels, counts)],\n    cbar=False,\n    annot_kws={\"color\": \"black\"},\n)\nax.set_xlabel(\"Predicted labes\")\nax.set_ylabel(\"True labels\")\nax.set_title(\"Confusion matrix\")\nplt.show()","key":"ZdPllBl7RZ"},{"type":"output","id":"UhdYt3Sq7bUgVSP9gDBsy","data":[],"key":"f0s2uioKnJ"}],"key":"ywnm4zpKun"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Plot time series according to the true label","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w0TgQ30ogM"}],"key":"MxIsJ45CCZ"}],"key":"W519IPvGxc"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"ncols = 3\nnrows = len(encoder.classes_) // ncols + 1\n\nfig, axs = plt.subplots(figsize=(15, 7), nrows=nrows, ncols=ncols)\n\nn, t, c = x_validate.shape\nplot_data = val_scaler.inverse_transform(x_validate.reshape(n * t, c)).reshape(n, t, c)[val_sample_mask]\nndvi = (plot_data[..., 3] - plot_data[..., 2]) / (plot_data[..., 3] + plot_data[..., 2])\n\nfor idx, (ax, cc) in enumerate(zip(axs.ravel(), encoder.classes_)):\n    ax.plot(ndvi[y_true == cc][:200].T, color=f\"C{idx}\", alpha=0.1)\n    ax.set_title(f\"Crop Type {cc} ({labels_dict[cc]})\")\n\nplt.tight_layout()","key":"Ah4sIC3YY5"},{"type":"output","id":"1DQDveKiSMGeugzyKzNNh","data":[],"key":"uETqm3Wspb"}],"key":"Eyb5eWJZGD"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Plot time series according to the predicted label","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"No64RxhyV2"}],"key":"z9NLQxPcli"}],"key":"hi0G9bP318"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"ncols = 3\nnrows = len(encoder.classes_) // ncols + 1\n\nfig, axs = plt.subplots(figsize=(15, 7), nrows=nrows, ncols=ncols)\n\nfor idx, (ax, cc) in enumerate(zip(axs.ravel(), encoder.classes_)):\n    ax.plot(ndvi[predictions == cc][:200].T, color=f\"C{idx}\", alpha=0.1)\n    ax.set_title(f\"Crop Type {cc} ({labels_dict[cc]})\")\n\nplt.tight_layout()","visibility":"show","key":"K1OvmtAMIm"},{"type":"output","id":"7Kr5qMCHSJUq2bjouXVK2","data":[],"visibility":"show","key":"XcCENGWuhu"}],"visibility":"show","key":"kyP0RcgJaR"}],"key":"EAk2Jfhc0r"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Demonstration of Eurocrops data use in geoDB","url":"/external-notebooks/eurodatacube/notebooks/notebooks/contributions/edc-geodb-4-eurocrops-demo","group":"Contributions"},"next":{"title":"Important notes","url":"/external-notebooks/eurodatacube/notebooks/notebooks/contributions/edc-usecase-lpis-crop-type-classification","group":"Contributions"}}},"domain":"http://localhost:3000"},"project":{"github":"https://github.com/eoxhub-workspaces/documentation/","id":"5aa0318b-5bc1-41e2-b1d4-053e8b61961b","exports":[],"bibliography":[],"title":"Location for eodashboard notebooks","index":"readme","pages":[{"title":"External Notebooks","level":1},{"title":"Eurodatacube","level":2},{"title":"Notebooks","level":3},{"slug":"external-notebooks.eurodatacube.notebooks.readme","title":"Jupyter notebooks","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":4},{"title":"Notebooks","level":4},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.readme","title":"Euro Data Cube Notebooks","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":5},{"title":"Contributions","level":5},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-ai4arctic-sea-ice","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-biomass-forest-observation-sys","title":"AIREO pilot dataset - Biomass (Forest Observation System)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-cap","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aireo-pilot-dataset-spacenet7","title":"Introduction","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.aot-covid19","title":"Aerosol Optical Thickness (AOT) Anomaly Detection Using Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.batch-zarr-output","title":"Generating Zarr data cubes with Sentinel Hub Batch API","description":"","date":"","thumbnail":"/build/68dffbf744fa42f4bf3d35e55684984e.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.client-side-processing-using-mass-sh","title":"Important notes","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.cmems-data-download","title":"Cmems Data Download","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.detect-trucks-sentinel2","title":"Detect Trucks using Sentinel-2 data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.drought-monitoring","title":"Drought impact monitoring platform","description":"","date":"","thumbnail":"/build/ad5c19782b006f72352ae57794305840.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-africa-cube-africa-custom-contest","title":"Custom Script Contest – Urban Growth in Africa","description":"","date":"","thumbnail":"/build/e9275f3fe80a704e2f5aa1ec1220e48a.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-byoc-tutorial","title":"How to bring your own data to EDC: Using Sentinel Hub Python package","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-demo-phi-week-v2","title":"xcube-gen and xcube-geodb Hands-On Workshop Φ-Week 2020","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-geodb-4-eurocrops-demo","title":"Demonstration of Eurocrops data use in geoDB","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-notebook-lstm","title":"South Africa Crop Type Classification on Euro Data Cube (EDC)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.edc-usecase-lpis-crop-type-classification","title":"Important notes","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.eurocrops-crop-classification-example","title":"Crop-classification using Sentinel-2 time-series","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.ghsl-urban-delineation","title":"\u003cb\u003eUrban delineation\u003cb\u003e","description":"","date":"","thumbnail":"/build/7bbf545a5eae1d9ebdb91ae42b261063.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.globalearthmonitor-example","title":"Pre-requisites","description":"","date":"","thumbnail":"/build/205c1ded6bd59771975a9fbd3753fd3b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.gpu-support-eoxhub","title":"GPU-Support on EOxHub","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.gran-chaco-phiweek","title":"ESA EO \\phi- week 2020","description":"","date":"","thumbnail":"/build/80cee7db6cdc7002ceedda5689f889fa.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.howtolulc-batch-updated-210621","title":"How To: Land-Use-Land-Cover Prediction for Slovenia - using the Batch Processing API","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.lpis-lulc-slo","title":"LPIS Use Case for Land Use / Land Cover Classification","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.lps2022-leverage-euro-data-cube-services","title":"Living Planet Symposium 2022 Training Session","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.no2-analysis-covid19-lockdowns","title":"Impact of Covid19 induced lockdown on NO2 levels","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.no2-so2-trend-veda-api-igarss-2023","title":"Using the NASA VEDA EOAPI","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.ogc-edc","title":"Important notes","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.openeo-client-demo","title":"openEO Client Demo","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.race-finishedgoodsinventory-inidcator","title":"Finished Goods Inventory indicator for RACE","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.third-party-planetscope","title":"Third Party Data Import - PlanetScope","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.xcube-access-datasets","title":"Using xcube to access non-commercial and commercial data sets","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.xcube-viewer-as-a-service-in-edc1","title":"Visualizing data with xcube viewer in EuroDataCube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-evaluation-workshop","title":"DAPA Evaluation Workshop: Introduction \u0026 Documentation","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-1-cube-sentinel-2","title":"DAPA Tutorial #1: Cube - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-2-area-sentinel-2","title":"DAPA Tutorial #2: Area - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-3-timeseries-sentinel-2","title":"DAPA Tutorial #3: Timeseries - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-4-value-sentinel-2","title":"DAPA Tutorial #4: Value - Sentinel-2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.dapa.dapa-tutorial-5-dem","title":"DAPA Tutorial #5: DEM","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.fairicube.fairicube-data-access-demonstration","title":"EDC Sentinel Hub - data access using xcube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-australian-bushfires","title":"Australian Bushfires","description":"","date":"","thumbnail":"/build/ab194b4f8c40a2b9d64d8a85e37ce1fd.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":["IGARSS-22","EO Dashboard"],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-data-access","title":"Data Access","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-detect-trucks-sentinel2","title":"Truck Detection Exercise","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-lockdown-in-venice","title":"Environmental Impacts of Lockdown in Venice, Italy during 2020","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-nasa-api-lockdown-in-venice","title":"Lockdown in Venice","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-no2-timeseries-analysis","title":"NO2 timeseries analysis","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.contributions.igarss2022.igarss-22-timeseries-stac-api-single-vs-multi","title":"Time series using STAC API statistics endpoints","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"title":"Curated","level":5},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.access-iacs-data","title":"How to access IACS Spatial Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-access-cci-data","title":"Access CCI data with xcube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-first-steps","title":"First steps on the Euro Data Cube platform","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-1-manage-datasets-v11","title":"Manage Collections in your GeoDB","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-2-explore-datasets-2","title":"Exploring Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-3-share-datasets1","title":"Sharing Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-geodb-access-urban-atlas-data-v1","title":"[xcube-geodb] How to accesss EEA Urban Atlas Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-data-access","title":"EDC Sentinel Hub - data access using xcube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinelhub-datafusion-basic","title":"EDC Sentinel Hub: Data Fusion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinelhub-datafusion-ndvi","title":"EDC Sentinel Hub: Data Fusion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-datasets","title":"EDC Sentinel Hub - using xcube to access different data sets in Sentinel Hub","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-setup","title":"Setup for EDC core API access using xcube client library","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-sentinel-hub-xcube-integration","title":"EDC Sentinel Hub - XCUBE integration","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.edc-usecase-ndvi-timeline","title":"NDVI timeline","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.eo-learn","title":"Using eo-learn in EDC: a starter’s guide.","description":"","date":"","thumbnail":"/build/5a1f8475f0c3652f4630d03d72891b98.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.esdl-edc-v0-2","title":"Earth System Data Lab Tutorial @ Euro Data Cube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.exploring-time-and-space-with-edc","title":"Exploring Time and Space: A guide to accessing, analysing and visualising data in the Euro Data Cube","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.how-to-access-dem-data-through-sentinel-hub-api","title":"How to access DEM data through Sentinel Hub API","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"slug":"external-notebooks.eurodatacube.notebooks.notebooks.curated.sentinelhub-py","title":"Using Sentinel Hub Process API in EDC: a starter’s guide.","description":"","date":"","thumbnail":"/build/01e22903839391bc356ac886ec2b2fa7.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":6},{"title":"Notebooks","level":1},{"slug":"notebooks.example1","title":"📘 Example Notebook 1","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.example2","title":"📘 Example Notebook 2","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/build/manifest-B4831781.js";
import * as route0 from "/build/root-QGLRP2PL.js";
import * as route1 from "/build/routes/$-7JYT5576.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/build/entry.client-UNPC4GT3.js");</script></body></html>