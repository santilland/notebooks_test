{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "API credentials have automatically been injected for your active subscriptions.  \n",
       "The following environment variables are now available:\n",
       "* `SH_INSTANCE_ID`\n",
       "* `SH_CLIENT_ID`\n",
       "* `SH_CLIENT_SECRET`\n",
       "* `SH_CLIENT_NAME`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from edc import setup_environment_variables\n",
    "setup_environment_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aerosol Optical Thickness (AOT) Anomaly Detection Using Sentinel-2\n",
    "---\n",
    "Author: Henrik Fisser, 2020\n",
    "\n",
    "This script was developed and executed on the EOxHub-hosted JupyterLab and uses xcube for computations on data cubes retrieved from the Sentinel Hub. To run this script you will need to ensure that you have access to these resources. \n",
    "\n",
    "__Important: This is a draft and was created during the COVID-19 crisis. Its purpose was to quickly implement an idea.__\n",
    "\n",
    "What does this script do?\n",
    "----\n",
    "During the COVID-19 crisis parts of the world were more or less locked up at home. The hypothesis is that these social distancing measures impact aerosol amounts in the troposphere. During the sen2cor (https://earth.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm) atmospheric correction and preprocessing of Sentinel-2 the aerosol amount is calculated as the dimensionless aerosol optical thickness (AOT). We use this data from computing temporal AOT medians on a Sentinel-2 stack from a baseline period and from the lockdown period, since the AOT is spatially and temporally highly variable. The reference period must be the same season from previous year. This script covers the respective period from 2017, 2018 and 2019. Finally, the percentage AOT difference is calculated. We call this difference the 'COVID-19 AOT anomaly'.\n",
    "\n",
    "Questions or Feedback?\n",
    "---\n",
    "Just send me a message :).\n",
    "\n",
    "Load dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcube_sh.cube import open_cube\n",
    "from xcube_sh.config import CubeConfig\n",
    "from xcube.core.maskset import MaskSet\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import shapely.geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General parameters\n",
    "---\n",
    "_run_batch_: shall we process everything?\\\n",
    "_dataset_: \"S2L2A\" for Sentinel-2 Level 2A\\\n",
    "_spatial_res_: spatial resolution. AOT will be 20 m anyways but VIS 10 m, so request 10\n",
    "_band_names_:\n",
    "for all years we need the AOT and the scene classification (SCL)\\\n",
    "_band_names_2020_: also request VIS bands for 2020 for visualization of the results.\\\n",
    "_time_period_before_: this is the period in which we consider an observation to be valid. As we request data from several months from three years for the reference period before COVID-19 we use a period of five days in order no to request and process too much data.\\\n",
    "_time_period_after_: the periods during COVID-19 lockdown-like measures is short, so use all available observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch = True # for batch processing\n",
    "dataset = \"S2L2A\"\n",
    "spatial_res = 0.00009 # 10m, AOT is 20 m though\n",
    "band_names = [\"AOT\", \"SCL\"]\n",
    "band_names_2020 = [\"B02\", \"B03\", \"B04\", \"AOT\", \"SCL\"]\n",
    "t = \"time\"\n",
    "\n",
    "# temporal parameters of data cubes\n",
    "time_period_before = \"5D\"\n",
    "time_period_after = \"1D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare areas of interest\n",
    "---\n",
    "Some areas are loaded from geojsons, others are provided as bounding boxes from coordinates in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read areas of interest\n",
    "def read_bbox(place):\n",
    "    aoi = gpd.read_file(os.path.expanduser(f\"~/.shared/notebooks/eurodatacube/notebooks/contributions/AOT_COVID-19/{place}.geojson\")).bounds.values[0]\n",
    "    return aoi[0].item(), aoi[1].item(), aoi[2].item(), aoi[3].item()\n",
    "\n",
    "place = \"beijing\"\n",
    "bbox_beijing = read_bbox(place)\n",
    "start_beijing = \"2020-02-09\"\n",
    "end_beijing = \"2020-03-15\"\n",
    "place = \"rome\"\n",
    "bbox_rome = 12.35, 41.79, 12.65, 41.99\n",
    "start_italy = \"2020-03-09\"\n",
    "place = \"madrid\"\n",
    "bbox_madrid = read_bbox(place)\n",
    "start_madrid = \"2020-03-14\"\n",
    "place = \"berlin\"\n",
    "bbox_berlin = read_bbox(place)\n",
    "place = \"london\"\n",
    "bbox_london = bbox = read_bbox(place)\n",
    "start_london = \"2020-03-23\"\n",
    "# NOTE: bucharest is currently disabled due to missing .geojson file\n",
    "# place = \"bucharest\"\n",
    "# bbox_bucharest = bbox = read_bbox(place)\n",
    "start_bucharest = \"2020-03-25\"\n",
    "place = \"paris\"\n",
    "bbox_paris = 2., 48.675, 2.6, 49.05\n",
    "place = \"brussels\"\n",
    "bbox_brussels = bbox = read_bbox(place)\n",
    "place = \"budapest\"\n",
    "bbox_budapest = bbox = read_bbox(place)\n",
    "place = \"prague\"\n",
    "bbox_prague = bbox = read_bbox(place)\n",
    "place = \"athens\"\n",
    "bbox_athens = 23.6, 37.935, 23.8, 38.05\n",
    "place = \"zurich\"\n",
    "bbox_zurich = bbox = read_bbox(place)\n",
    "\n",
    "bboxes = {\"beijing\":bbox_beijing, \"rome\":bbox_rome, \"madrid\":bbox_madrid, \n",
    "          \"berlin\":bbox_berlin, \"london\":bbox_london, # \"bucharest\":bbox_bucharest, \n",
    "          \"paris\":bbox_paris, \"brussels\":bbox_brussels, \"budapest\":bbox_budapest, \n",
    "          \"prague\":bbox_prague, \"athens\":bbox_athens, \"zurich\":bbox_zurich}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process area-wise\n",
    "---\n",
    "This is scripted for batch processing. If you would like to test it e.g. only in one area just provide a single place name and the corresponding aoi in the bboxes dictionary.\\\n",
    "Since this is a very functional draft script, the handling of different lockdown period is kind of messy. Essentially, due to area-specific lockdown periods we have to provide different temporal bounds for the cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_batch:\n",
    "    for place in bboxes:\n",
    "        print(\"Processing: \" + place)\n",
    "        if not os.path.exists(place):\n",
    "            is_beijing = place == \"beijing\"\n",
    "            is_london = place == \"london\"\n",
    "            is_italy = place in [\"rome\", \"milan\"]\n",
    "            is_madrid = place == \"madrid\"\n",
    "            is_vienna = place == \"vienna\"\n",
    "            is_bucharest = place == \"bucharest\"\n",
    "\n",
    "            bbox = bboxes.get(place)\n",
    "\n",
    "            # cube 2017\n",
    "            start = \"2017-03-04\"\n",
    "            end = \"2017-05-05\"\n",
    "            start = \"2017-01-26\" if is_beijing else start\n",
    "            end = \"2017-03-29\" if is_beijing else end\n",
    "            start = \"2017-02-24\" if is_italy else start\n",
    "            start = \"2017-03-09\" if is_london else start\n",
    "            start = \"2017-03-01\" if is_madrid else start\n",
    "            start = \"2017-03-11\" if is_bucharest else start\n",
    "            cube_config_2017 = CubeConfig(dataset_name = dataset,\n",
    "                                        band_names = band_names,\n",
    "                                        tile_size = [512, 512],\n",
    "                                        geometry = bbox,\n",
    "                                        spatial_res = spatial_res,\n",
    "                                        time_range = [start, end],\n",
    "                                        time_period = time_period_before)\n",
    "            cube_2017 = open_cube(cube_config_2017)\n",
    "            scl = MaskSet(cube_2017.SCL)\n",
    "            cube_2017_masked = cube_2017.where((scl.no_data + \n",
    "                                                scl.cloud_shadows +\n",
    "                                                scl.clouds_high_probability + \n",
    "                                                scl.clouds_medium_probability) == 0)\n",
    "            cube_2017_masked = cube_2017_masked.drop_vars([\"SCL\"])\n",
    "\n",
    "            # cube 2018\n",
    "            start = \"2018-03-04\"\n",
    "            end = \"2018-05-05\"\n",
    "            start = \"2018-01-26\" if is_beijing else start\n",
    "            end = \"2018-03-29\" if is_beijing else end\n",
    "            start = \"2018-02-24\" if is_italy else start\n",
    "            start = \"2018-03-09\" if is_london else start\n",
    "            start = \"2018-03-01\" if is_madrid else start\n",
    "            start = \"2018-03-11\" if is_bucharest else start\n",
    "            cube_config_2018 = CubeConfig(dataset_name = dataset,\n",
    "                                        band_names = band_names,\n",
    "                                        tile_size = [512, 512],\n",
    "                                        geometry = bbox,\n",
    "                                        spatial_res = spatial_res,\n",
    "                                        time_range = [start, end],\n",
    "                                        time_period = time_period_before)\n",
    "            cube_2018 = open_cube(cube_config_2018)\n",
    "            scl = MaskSet(cube_2018.SCL)\n",
    "            cube_2018_masked = cube_2018.where((scl.no_data + \n",
    "                                                scl.cloud_shadows +\n",
    "                                                scl.clouds_high_probability + \n",
    "                                                scl.clouds_medium_probability) == 0)\n",
    "            cube_2018_masked = cube_2018_masked.drop_vars([\"SCL\"])\n",
    "\n",
    "            # cube 2019\n",
    "            start = \"2019-03-04\"\n",
    "            end = \"2019-05-05\"\n",
    "            start = \"2019-01-26\" if is_beijing else start\n",
    "            end = \"2019-03-29\" if is_beijing else end\n",
    "            start = \"2019-02-24\" if is_italy else start\n",
    "            start = \"2019-03-09\" if is_london else start\n",
    "            start = \"2019-03-01\" if is_madrid else start\n",
    "            start = \"2019-03-11\" if is_bucharest else start\n",
    "            cube_config_2019 = CubeConfig(dataset_name = dataset,\n",
    "                                         band_names = band_names,\n",
    "                                         tile_size = [512, 512],\n",
    "                                         geometry = bbox,\n",
    "                                         spatial_res = spatial_res,\n",
    "                                         time_range = [start, end],\n",
    "                                         time_period = time_period_before)\n",
    "            cube_2019 = open_cube(cube_config_2019)\n",
    "            scl = MaskSet(cube_2019.SCL)\n",
    "            cube_2019_masked = cube_2019.where((scl.no_data + \n",
    "                                                scl.cloud_shadows +\n",
    "                                                scl.clouds_high_probability + \n",
    "                                                scl.clouds_medium_probability) == 0)\n",
    "            cube_2019_masked = cube_2019_masked.drop_vars([\"SCL\"])\n",
    "\n",
    "            # median AOT 2017, 2018 and 2019\n",
    "            cube_171819 = xr.merge([cube_2017_masked, cube_2018_masked, cube_2019_masked])\n",
    "            median_aot_171819 = cube_171819.AOT.median(dim = t)\n",
    "\n",
    "            # cube 2020\n",
    "            start = \"2020-03-18\"\n",
    "            end = \"2020-04-21\"\n",
    "            start = start_italy if is_italy else start\n",
    "            start = start_beijing if is_beijing else start\n",
    "            start = start_london if is_london else start\n",
    "            start = start_madrid if is_madrid else start\n",
    "            start = start_bucharest if is_bucharest else start\n",
    "            end = end_beijing if is_beijing else end\n",
    "            cube_config_2020 = CubeConfig(dataset_name = dataset,\n",
    "                                         band_names = band_names_2020,\n",
    "                                         tile_size = [512, 512],\n",
    "                                         geometry = bbox,\n",
    "                                         spatial_res = spatial_res,\n",
    "                                         time_range = [start, end],\n",
    "                                         time_period = \"1D\")\n",
    "            cube_2020 = open_cube(cube_config_2020)\n",
    "            scl = MaskSet(cube_2020.SCL)\n",
    "            cube_2020_masked = cube_2020.where((scl.no_data + \n",
    "                                                scl.cloud_shadows +\n",
    "                                                scl.clouds_high_probability + \n",
    "                                                scl.clouds_medium_probability) == 0)\n",
    "            median_aot_2020 = cube_2020_masked.AOT.median(dim = t)\n",
    "\n",
    "            # percentage difference\n",
    "            diff_aot_percent = ((median_aot_2020 / median_aot_171819) * 100) - 100\n",
    "\n",
    "            # median RGB for visualization\n",
    "            median_red = cube_2020_masked.B04.median(dim = t)\n",
    "            median_green = cube_2020_masked.B03.median(dim = t)\n",
    "            median_blue = cube_2020_masked.B02.median(dim = t)\n",
    "\n",
    "            # write results\n",
    "            os.mkdir(place)\n",
    "            suffix = \"_\" + place + \".nc\"\n",
    "            diff_aot_percent.to_netcdf(os.path.join(place, \"aot_diff_percent\" + suffix))\n",
    "            median_aot_2020.to_netcdf(os.path.join(place, \"median_aot_2020\" + suffix))\n",
    "            median_aot_171819.to_netcdf(os.path.join(place, \"median_aot_171819\" + suffix))\n",
    "            median_red.to_netcdf(os.path.join(place, \"B04\" + suffix))\n",
    "            median_green.to_netcdf(os.path.join(place, \"B03\" + suffix))\n",
    "            median_blue.to_netcdf(os.path.join(place, \"B02\" + suffix))\n",
    "            print(\"Done with: \" + place)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "properties": {
   "automatically_executable": false,
   "authors": [
    {"name": "henrik.fisser@t-online.de", "id": "b62330c3-ed93-4b41-a90f-28f3a73107e9"}
   ],
   "description": "AOT anomaly analysis during COVID-19 measures",
   "id": "e8d9d46d-5375-4a07-b185-b8195f831f30",
   "license": null,
   "name": "AOT anomaly analysis during COVID-19 measures",
   "requirements": [
    "eurodatacube"
   ],
   "tags": [
    "COVID-19",
    "Sentinel Hub"
   ],
   "tosAgree": true,
   "type": "Jupyter Notebook",
   "version": "0.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
